{"cells":[{"cell_type":"markdown","source":["## Logistic Regression on TF-IDF\n","### Training Pipeline"],"metadata":{"colab_type":"text","id":"lbCPRMjSjxAD","cell_id":"a6f0a381-315e-4fc1-b875-94f7e237e29e"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"155231e4-e37c-4dcf-998b-4890ae38ad71"},"source":["# !pip install dagshub"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"YBkPvqU1iJMY","cell_id":"195e66df-de66-4b91-b39c-f04452675145"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","\n","import dagshub\n","import pickle"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"5dfb02ba-2c84-4d2c-85f1-15f69184fa30"},"source":["def load_corpus(DATASET_PATH, CODE_COLUMN):\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8', comment='#', sep='\\t')#, quoting=csv.QUOTE_NONE, error_bad_lines=False)#, sep=','\n","    print(df.head())\n","    corpus = df[CODE_COLUMN]\n","    test_size = 0.1\n","    test_rows = round(df.shape[0]*test_size)\n","    train_rows = df.shape[0] - test_rows\n","    train_corpus = df[CODE_COLUMN][0:test_rows]\n","    test_corpus = df[CODE_COLUMN][train_rows:]\n","    return df, corpus"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"colab_type":"code","id":"rkZxucYdjxAd","outputId":"80421fe6-ac85-496a-fa5e-064c917120b5","cell_id":"fa13f41c-1a25-4e14-a575-564bb1fe77cc"},"source":["def tfidf_transform(corpus, tfidf_params, TFIDF_DIR):\n","#     tfidf = TfidfVectorizer(min_df=5\n","#                             , max_df = 0.3\n","#                             , ngram_range = (1,2)\n","#                             , smooth_idf = True\n","#                            )\n","    # tfidf = TfidfVectorizer(tfidf_params)\n","    # features = tfidf.fit_transform(corpus)\n","    tfidf = pickle.load(open(TFIDF_DIR, 'rb'))\n","    features = tfidf.transform(corpus)\n","    return features"],"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR):\n","    tfidf = TfidfVectorizer(tfidf_params)\n","    tfidf = tfidf.fit(code_blocks)\n","    pickle.dump(tfidf, open(TFIDF_DIR, \"wb\"))\n","    code_blocks_tfidf = tfidf.transform(code_blocks)\n","    return code_blocks_tfidf"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"-clcrT5-jxAl","scrolled":true,"cell_id":"3041592d-1fa0-44c3-93b5-3a0683be87e1"},"source":["def logreg_evaluate(df, code_blocks, TAG_TO_PREDICT):\n","    code_blocks_tfidf = tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR)\n","    X_train, X_test, y_train, y_test = train_test_split(code_blocks_tfidf, df[TAG_TO_PREDICT], test_size=0.25)\n","    clf = LogisticRegression(random_state=421).fit(X_train, y_train)\n","    # clf.fit(X_train, y_train)\n","    print(\"saving the model\")\n","    pickle.dump(clf, open(MODEL_DIR, 'wb'))\n","    y_pred = clf.predict(X_test)\n","    accuracy = clf.score(X_test, y_test)\n","    f1 = f1_score(y_pred, y_test)\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","    print(f'F1-score {round(f1*100, 2)}%')\n","    errors = y_test - y_pred\n","    plt.hist(errors)\n","    plot_precision_recall_curve(clf, X_test, y_test)\n","    plot_confusion_matrix(clf, X_test, y_test, values_format='d')\n","    def mean_confidence_interval(data, confidence=0.95):\n","        a = 1.0 * np.array(data)\n","        n = len(a)\n","        m, se = np.mean(a), scipy.stats.sem(a)\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","        return m, m-h, m+h\n","    conf_interval = mean_confidence_interval(errors, 0.95)\n","    print(conf_interval)\n","    metrics = {'test_accuracy': accuracy\n","               , 'test_f1_score': f1}\n","    return metrics"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"42e2ba63-38e5-444d-a9f3-0f26776f3b63"},"source":["def get_predictions(X, y, TAG_TO_PREDICT, MODEL_DIR):\r\n","    clf = pickle.load(open(MODEL_DIR, 'rb'))\r\n","    # result = loaded_model.score(X, y)\r\n","    y_pred = clf.predict(X)\r\n","    accuracy = accuracy_score(y_pred, y)\r\n","    f1 = f1_score(y_pred, y, average='weighted')\r\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\r\n","    print(f'F1-score {round(f1*100, 2)}%')\r\n","    errors = y - y_pred\r\n","    plt.hist(errors)\r\n","    plot_precision_recall_curve(clf, X, y)\r\n","    plot_confusion_matrix(clf, X, y, values_format='d')\r\n","    def mean_confidence_interval(data, confidence=0.95):\r\n","        a = 1.0 * np.array(data)\r\n","        n = len(a)\r\n","        m, se = np.mean(a), scipy.stats.sem(a)\r\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\r\n","        return m, m-h, m+h\r\n","    conf_interval = mean_confidence_interval(errors, 0.95)\r\n","    print(conf_interval)\r\n","    metrics = {'test_accuracy': accuracy\r\n","               , 'test_f1_score': f1}\r\n","    return metrics"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"d1rIH7pBtjFq","cell_id":"7dc1bcd5-fe3e-49f8-8211-62d7f8ddf438","output_cleared":false,"tags":[]},"source":["if __name__ == '__main__':\n","    DATASET_PATH = './data/code_blocks_regex_graph_v2.2.csv'\n","    MODEL_DIR = './models/logreg_regex_graph_v2.2.sav'\n","    TFIDF_DIR = './models/tfidf_logreg_graph_v2.2.pickle'\n","    CODE_COLUMN = 'code_block'\n","    TAG_TO_PREDICT = 'preprocessing'\n","    SCRIPT_DIR = 'logreg_classifier.ipynb'\n","\n","    df, code_blocks = load_corpus(DATASET_PATH, CODE_COLUMN)\n","    nrows = df.shape[0]\n","    print(\"loaded\")\n","    tfidf_params = {'min_df': 5\n","                    , 'max_df': 0.3\n","                    , 'smooth_idf': True}\n","    data_meta = {'DATASET_PATH': DATASET_PATH\n","                ,'nrows': nrows\n","                ,'label': TAG_TO_PREDICT\n","                ,'model': MODEL_DIR\n","                ,'script_dir': SCRIPT_DIR}\n","    print(\"tfidf-ed\")\n","    with dagshub.dagshub_logger() as logger:\n","        metrics = logreg_evaluate(df, code_blocks, TAG_TO_PREDICT)\n","        # metrics = get_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\n","        logger.log_hyperparams(data_meta)\n","        logger.log_hyperparams(tfidf_params)\n","        logger.log_metrics(metrics)\n","    print(\"finished\")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## Errors Analysis"]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[]},"outputs":[],"source":["# def analyze_predictions(X, y, TAG_TO_PREDICT, MODEL_DIR):\n","#     clf = pickle.load(open(MODEL_DIR, 'rb'))\n","#     # result = loaded_model.score(X, y)\n","#     y_pred = clf.predict(X)\n","#     accuracy = accuracy_score(y_pred, y)\n","#     f1 = f1_score(y_pred, y, average='weighted')\n","#     print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","#     print(f'F1-score {round(f1*100, 2)}%')\n","#     errors = y - y_pred\n","#     plt.hist(errors)\n","#     plot_precision_recall_curve(clf, X, y)\n","#     plot_confusion_matrix(clf, X, y, values_format='d')\n","#     def mean_confidence_interval(data, confidence=0.95):\n","#         a = 1.0 * np.array(data)\n","#         n = len(a)\n","#         m, se = np.mean(a), scipy.stats.sem(a)\n","#         h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","#         return m, m-h, m+h\n","#     conf_interval = mean_confidence_interval(errors, 0.95)\n","#     print(conf_interval)\n","#     metrics = {'test_accuracy': accuracy\n","#                , 'test_f1_score': f1}\n","#     return X, y, y_pred\n","# if __name__ == '__main__':\n","#     DATASET_PATH = './data/chunks_10_validate.csv'\n","#     MODEL_DIR = './models/logreg_regex_graph_v2.sav'\n","#     TFIDF_DIR = './models/tfidf_logreg_graph_v2.pickle'\n","#     CODE_COLUMN = 'code'\n","#     TAG_TO_PREDICT = 'tag'\n","#     df, corpus = load_corpus(DATASET_PATH, CODE_COLUMN)\n","#     nrows = df.shape[0]\n","#     print(\"loaded\")\n","#     params = {'min_df': 5\n","#              , 'max_df': 0.3\n","#              , 'smooth_idf': True}\n","#     data_meta = {'DATASET_PATH': DATASET_PATH\n","#                 ,'nrows': nrows\n","#                 ,'label': TAG_TO_PREDICT\n","#                 ,'model': 'Logistic Regression'}\n","#     features = tfidf_transform(corpus, params, TFIDF_DIR)\n","#     print(\"tfidf-ed\")\n","#     X, y, y_pred = analyze_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\n","#     print(\"finished\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# pd.set_option('max_colwidth', 500)\n","# pd.set_option('max_rows', 500)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# ## False Positives (preprocessing y/n)\n","# corpus[(y_pred-y)==-1].reset_index(drop=True)[:]"]},{"cell_type":"markdown","metadata":{},"source":["## Weights Analysis"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# from operator import itemgetter\n","# def show_most_informative_features(model, vectorizer=None, text=None, n=20):\n","#     \"\"\"\n","#     Extract the vectorizer and the classifier from the pipeline\n","#     \"\"\"\n","#     if vectorizer is None:\n","#         vectorizer = model.named_steps['vectorizer']\n","#     else:\n","#         text = vectorizer.transform([text])\n","\n","#     classifier = model#.named_steps['classifier']\n","#     feat_names = vectorizer.get_feature_names()\n","\n","#     # Check to make sure that we can perform this computation\n","#     if not hasattr(classifier, 'coef_'):\n","#         raise TypeError(\n","#             \"Cannot compute most informative features on {}.\".format(\n","#                 classifier.__class__.__name__\n","#             )\n","#         )    \n","\n","#     # Otherwise simply use the coefficients\n","#     tvec = classifier.coef_\n","\n","#     # Zip the feature names with the coefs and sort   \n","#     coefs = sorted(\n","#         zip(tvec[0], feat_names),\n","#         key=itemgetter(0), reverse=True\n","#     )\n","\n","#     # Get the top n and bottom n coef, name pairs\n","#     topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n","\n","#     # Create the output string to return\n","#     output = []\n","#     # output = pd.DataFrame()\n","#     # If text, add the predicted value to the output.\n","#     if text is not None:\n","#         # output.append(\"\\\"{}\\\"\".format(text))\n","#         # output.append(\n","#         #     \"Classified as: {}\".format(model.predict(text))\n","#         # )\n","#         # output.append(\"\")\n","#         print(\"Classified as: {}\".format(model.predict(text)))\n","#     # Create two columns with most negative and most positive features.\n","\n","#     for (cp, fnp), (cn, fnn) in topn:\n","#         print(cp, fnp, cn, fnn)\n","#         output.append(\n","#             \"{:0.2f}{: >15}    {:0.2f}{: >15}\".format(\n","#                 cp, fnp, cn, fnn\n","#             )\n","#         )\n","\n","#     return \"\\n\".join(output)"]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[]},"outputs":[],"source":["# MODEL_DIR = './models/logreg_regex_graph_v2.2.sav'\n","# TFIDF_DIR = './models/tfidf_logreg_graph_v2.2.pickle'\n","# DATASET_PATH = './data/chunks_5_validate.csv'\n","# CODE_COLUMN = 'code'\n","# TAG_TO_PREDICT = 'tag'\n","# df, corpus = load_corpus(DATASET_PATH, CODE_COLUMN)\n","# interpret = show_most_informative_features(model=pickle.load(open(MODEL_DIR, 'rb')),\n","#                                 vectorizer=pickle.load(open(TFIDF_DIR, 'rb')),\n","#                                 text=corpus[5],\n","#                                 n = 500)\n","# # with open('logreg_interpret.txt', mode='w') as f:\n","# #     f.write(interpret)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"colab":{"collapsed_sections":[],"name":"NL2ML: LogReg.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0-final"},"deepnote_notebook_id":"2fa6153d-52cd-43ae-af2d-05210d5785e2","deepnote_execution_queue":[]}}