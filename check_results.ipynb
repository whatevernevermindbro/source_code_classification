{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_VER = 5\n",
    "MODEL = 'logreg' #'logreg' or 'svm'\n",
    "TAGS_TO_PREDICT = ['import', 'data_import', 'data_export', 'preprocessing',\n",
    "                    'visualization', 'model', 'train', 'predict']\n",
    "DATASET_PATH = \"./data/github_chunks_10_{}_{}_v{}.csv\".format(TAGS_TO_PREDICT, MODEL, GRAPH_VER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ", \n              kernel_size=(3,), \n              strides=1,\n              padding='same',               activation='relu')\nz = conv(x)\nW_conv, b_conv = conv.weights\nprint (f\"x: {x.shape}\")\nprint (f\"W_conv: {W_conv.shape}\")\nprint (f\"z: {z.shape}\")\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\npool = GlobalMaxPool1D(data_format='channels_last')\nz = pool(z)\nprint (f\"z: {z.shape}\")\nfrom tensorflow.keras.layers import BatchNormalization\nbatch_norm = BatchNormalization()\nz = batch_norm(conv(x)) print (f\"z: {z.shape}\")\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv1D\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nclass CNN(Model):\n    def __init__(self, filter_size, num_filters,\n                 hidden_dim, dropout_p, num_classes):\n        super(CNN, self).__init__(name=\"cnn\")\n        \n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n        self.relu = Activation('relu')\n        self.batch_norm = BatchNormalization()\n        self.pool = GlobalMaxPool1D(data_format='channels_last')\n\n                self.fc1 = Dense(units=hidden_dim, activation='relu')\n        self.dropout = Dropout(rate=dropout_p)\n        self.fc2 = Dense(units=num_classes, activation='softmax')\n\n    def call(self, x_in, training=False):\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n                x_in = tf.cast(x_in, tf.float32)\n\n                z = self.conv(x_in)\n        z = self.relu(z)\n        z = self.batch_norm(z)\n        z = self.pool(z)\n\n                z = self.fc1(z)\n        z = self.dropout(z, training=training)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n\n        return y_pred\n\n    def summary(self, input_shape):\n        x_in = Input(shape=input_shape, name='X')\n        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)\n        return plot_model(summary, show_shapes=True) \nmodel = CNN(filter_size=FILTER_SIZE,\n            num_filters=NUM_FILTERS,\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n            dropout_p=DROPOUT_P,\n            num_classes=NUM_CLASSES)\nvocab_size = len(X_tokenizer.word_index) + 1 model.summary(input_shape=(max_sequence_size, vocab_size,))\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  1\nPREDICT:  0\n=============================\n=============================\n%load_ext tensorboard\ncallbacks = [EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_CRITERIA, verbose=1, mode='min'),\n             ReduceLROnPlateau(patience=1, factor=0.1, verbose=0),\n             TensorBoard(log_dir='tensorboard', histogram_freq=1, update_freq='epoch')]\ntraining_history = model.fit(x=training_generator,\n                             epochs=NUM_EPOCHS,\n                             validation_data=validation_generator,\n                             callbacks=callbacks,\n                             shuffle=False,\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  1\nPREDICT:  0\n=============================\n=============================\n                             verbose=1)\ndef get_performance(y_true, y_pred, classes):\n    \n    performance = {'overall': {}, 'class': {}}\n    y_pred = np.argmax(y_pred, axis=1)\n    metrics = precision_recall_fscore_support(y_true, y_pred)\n\n        performance['overall']['precision'] = np.mean(metrics[0])\n    performance['overall']['recall'] = np.mean(metrics[1])\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  1\n=============================\n=============================\n    performance['overall']['num_samples'] = np.float64(np.sum(metrics[3]))\n\n        for i in range(len(classes)):\n        performance['class'][classes[i]] = {\n            \"precision\": metrics[0][i],\n            \"recall\": metrics[1][i],\n            \"f1\": metrics[2][i],\n            \"num_samples\": np.float64(metrics[3][i])\n        }\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  1\n=============================\n=============================\n    return performance\ntest_history = model.evaluate(x=testing_generator, verbose=1)\ny_pred = model.predict(x=testing_generator, verbose=1)\nprint (f\"test history: {test_history}\")\nperformance = get_performance(y_true=y_test, y_pred=y_pred, classes=classes)\nprint (json.dumps(performance, indent=4))\nplt.rcParams[\"figure.figsize\"] = (7,7)\ny_pred = np.argmax(y_pred, axis=1)\nplot_confusion_matrix(y_test, y_pred, classes=classes)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  1\n=============================\n=============================\ninference_generator = DataGenerator(X=X_infer,\n                                    y=y_filler,\n                                    batch_size=len(y_filler),\n                                    vocab_size=vocab_size,\n                                    max_filter_size=FILTER_SIZE,\n                                    shuffle=False)\ny_prob = model.predict(x=inference_generator, verbose=1)\nresults = []\nfor index in range(num_samples):\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  1\n=============================\n=============================\n        'raw_input': texts[index],\n        'preprocessed_input': decode_one_hot(seq=X_infer[index], tokenizer=X_tokenizer),\n        'probabilities': get_probability_distribution(y_prob[index], y_tokenizer.classes_)\n                   })\nprint (json.dumps(results, indent=4))\nurl = \"https://raw.githubusercontent.com/madewithml/basics/master/data/harrypotter.txt\"\nresponse = urllib.request.urlopen(url)\nhtml = response.read()\nwith open(DATA_FILE, 'wb') as fp:\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nwith open(DATA_FILE, encoding='cp1252') as fp:\n    book = fp.read()\nsentences = tokenizer.tokenize(book)\nprint (f\"{len(sentences)} sentences\")\nprint (sentences[11])\nsentences = [text_to_word_sequence(\n    text=sentence, filters=FILTERS, \n    lower=LOWER, split=' ') for sentence in sentences]\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nimport gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nEMBEDDING_DIM = 100\nWINDOW = 5\nMIN_COUNT = 3 SKIP_GRAM = 1 NEGATIVE_SAMPLING = 20\nw2v = Word2Vec(sentences=sentences, size=EMBEDDING_DIM, \n               window=WINDOW, min_count=MIN_COUNT, \n               sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nw2v.wv.get_vector(\"potter\")\nw2v.wv.most_similar(positive=\"scar\", topn=5)\nw2v.wv.save_word2vec_format('model.bin', binary=True)\nw2v = KeyedVectors.load_word2vec_format('model.bin', binary=True)\nfrom gensim.models import FastText\nft = FastText(sentences=sentences, size=EMBEDDING_DIM, \n              window=WINDOW, min_count=MIN_COUNT, \n              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\nprint (ft)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nft.wv.save('model.bin')\nft = KeyedVectors.load('model.bin')\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nresp = urlopen('http://nlp.stanford.edu/data/glove.6B.zip')\nzipfile = ZipFile(BytesIO(resp.read()))\nzipfile.namelist()\nembeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\nzipfile.extract(embeddings_file)\nwith open(embeddings_file, 'r') as fp:\n    line = next(fp)\n    values = line.split()\n    word = values[0]\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n    print (f\"word: {word}\")\n    print (f\"embedding:\\n{embedding}\")\n    print (f\"embedding dim: {len(embedding)}\")\nword2vec_output_file = '{0}.word2vec'.format(embeddings_file)\nglove2word2vec(embeddings_file, word2vec_output_file)\nglove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nglove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\nglove.wv.most_similar(positive=\"goku\", topn=5)\nX = glove[glove.wv.vocab]\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\npca_results = pca.fit_transform(X)\nplot_embeddings(words=[\"king\", \"queen\", \"man\", \"woman\"], \n                embeddings=glove, \n                pca_results=pca_results)\nglove.most_similar(positive=['woman', 'doctor'], negative=['man'], topn=5)\nimport pandas as pd\nimport re\nimport urllib\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  1\nPREDICT:  0\n=============================\n=============================\nX_tokenizer = Tokenizer(\n    filters=FILTERS, lower=LOWER, char_level=CHAR_LEVEL, oov_token='<UNK>')\nimport math\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nclass TextDataset(Dataset):\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n    def __init__(self, X, y, batch_size, max_filter_size):\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.max_filter_size = max_filter_size\n    \n    def __str__(self):\n        return f\"<Dataset(N={len(self)}, batch_size={self.batch_size}, num_batches={self.get_num_batches()})>\"\n\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n        return len(self.y)\n\n    def __getitem__(self, index):\n        X = self.X[index]\n        y = self.y[index]\n        return X, y\n\n    def get_num_batches(self):\n        return math.ceil(len(self)/self.batch_size)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n    def collate_fn(self, batch):\n        \n                X = np.array(batch)[:, 0]\n        y = np.array(batch)[:, 1]\n\n                max_seq_len = max(self.max_filter_size, max([len(x) for x in X]))\n        X = pad_sequences(X, padding=\"post\", maxlen=max_seq_len)\n\n                X = torch.LongTensor(X.astype(np.int32))\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n\n        return X, y\n\n    def generate_batches(self, shuffle=False, drop_last=False):\n        dataloader = DataLoader(dataset=self, batch_size=self.batch_size, \n                                collate_fn=self.collate_fn, \n                                shuffle=shuffle, drop_last=drop_last)\n        for (X, y) in dataloader:\n            yield X, y\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nval_set = TextDataset(X=X_val, y=y_val, batch_size=BATCH_SIZE, max_filter_size=max(FILTER_SIZES))\ntest_set = TextDataset(X=X_test, y=y_test, batch_size=BATCH_SIZE, max_filter_size=max(FILTER_SIZES))\nprint (train_set)\nprint (train_set[0])\nimport torch\nimport torch.nn as nn\nx = torch.randint(high=10, size=(1,5)) print (x)\nprint (x.shape)\nembeddings = nn.Embedding(embedding_dim=100,\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nembeddings(x).shape\nimport torch.nn.functional as F\nclass CNN(nn.Module):\n    def __init__(self, embedding_dim, vocab_size, num_filters, \n                 filter_sizes, hidden_dim, dropout_p, num_classes, \n                 pretrained_embeddings=None, freeze_embeddings=False,\n                 padding_idx=0):\n        super(CNN, self).__init__()\n\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n        \n                if pretrained_embeddings is None:\n            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n                                          num_embeddings=vocab_size,\n                                          padding_idx=padding_idx)\n        else:\n            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n                                           num_embeddings=vocab_size,\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n                                           _weight=pretrained_embeddings)\n        \n                if freeze_embeddings:\n            self.embeddings.weight.requires_grad = False\n        \n                self.conv = nn.ModuleList(\n            [nn.Conv1d(in_channels=embedding_dim, \n                       out_channels=num_filters, \n                       kernel_size=f) for f in filter_sizes])\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n                self.dropout = nn.Dropout(dropout_p)\n        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x_in, channel_first=False, apply_softmax=False):\n        \n                x_in = self.embeddings(x_in)\n\n                if not channel_first:\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n            \n                z = []\n        max_seq_len = x_in.shape[2]\n        for i, f in enumerate(self.filter_sizes):\n                        padding_left = int((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)\n            padding_right = int(math.ceil((self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))\n\n                        _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))\n            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n        \n                z = torch.cat(z, 1)\n\n                z = self.fc1(z)\n        z = self.dropout(z)\n        y_pred = self.fc2(z)\n        \n        if apply_softmax:\n            y_pred = F.softmax(y_pred, dim=1)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\ndef load_glove_embeddings(embeddings_file):\n    \n    embeddings = {}\n    with open(embeddings_file, \"r\") as fp:\n        for index, line in enumerate(fp):\n            values = line.split()\n            word = values[0]\n            embedding = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = embedding\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\ndef make_embeddings_matrix(embeddings, word_index, embedding_dim):\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\nembeddings_file = 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nembedding_matrix = make_embeddings_matrix(embeddings=glove_embeddings, \n                                          word_index=X_tokenizer.word_index, \n                                          embedding_dim=EMBEDDING_DIM)\nprint (f\"<Embeddings(words={embedding_matrix.shape[0]}, dim={embedding_matrix.shape[1]})>\")\nimport matplotlib.pyplot as plt\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchsummary import summary\n-----------------------------\nIMPORT:  1\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\nFREEZE_EMBEDDINGS = True\nglove_frozen_model = CNN(embedding_dim=EMBEDDING_DIM,\n                         vocab_size=vocab_size,\n                         num_filters=NUM_FILTERS,\n                         filter_sizes=FILTER_SIZES,\n                         hidden_dim=HIDDEN_DIM,\n                         dropout_p=DROPOUT_P,\n                         num_classes=len(classes),\n                         pretrained_embeddings=embedding_matrix,\n-----------------------------\nIMPORT:  0\nDATA IMPORT:  0\nDATA EXPORT:  0\nVISUALIZATION:  0\nMODEL:  0\nTRAIN:  0\nPREDICT:  0\n=============================\n=============================\n"
    }
   ],
   "source": [
    "for i in range(1000, 1100):#len(results)):\n",
    "    print(results.loc[i, '0'])\n",
    "    print('-----------------------------')\n",
    "    print('IMPORT: ', results.loc[i, 'import'])\n",
    "    print('DATA IMPORT: ', results.loc[i, 'data_import'])\n",
    "    print('DATA EXPORT: ', results.loc[i, 'data_export'])\n",
    "    print('VISUALIZATION: ', results.loc[i, 'visualization'])\n",
    "    print('MODEL: ', results.loc[i, 'model'])\n",
    "    print('TRAIN: ', results.loc[i, 'train'])\n",
    "    print('PREDICT: ', results.loc[i, 'predict'])\n",
    "    print('=============================')\n",
    "    print('=============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}