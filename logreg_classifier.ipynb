{"cells":[{"cell_type":"markdown","source":["### TF-IDF + Log Reg"],"metadata":{"colab_type":"text","id":"lbCPRMjSjxAD","cell_id":"a6f0a381-315e-4fc1-b875-94f7e237e29e"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"155231e4-e37c-4dcf-998b-4890ae38ad71"},"source":["# !pip install dagshub"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"YBkPvqU1iJMY","cell_id":"195e66df-de66-4b91-b39c-f04452675145"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","\n","import dagshub\n","import pickle"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"5dfb02ba-2c84-4d2c-85f1-15f69184fa30"},"source":["def load_corpus(DATASET_PATH, CODE_COLUMN):\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8', comment='#', sep=',')#, quoting=csv.QUOTE_NONE, error_bad_lines=False)#, sep=','\n","    print(df.head())\n","    corpus = df[CODE_COLUMN]\n","    test_size = 0.1\n","    test_rows = round(df.shape[0]*test_size)\n","    train_rows = df.shape[0] - test_rows\n","    train_corpus = df[CODE_COLUMN][0:test_rows]\n","    test_corpus = df[CODE_COLUMN][train_rows:]\n","    return df, corpus"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"colab_type":"code","id":"rkZxucYdjxAd","outputId":"80421fe6-ac85-496a-fa5e-064c917120b5","cell_id":"fa13f41c-1a25-4e14-a575-564bb1fe77cc"},"source":["def tfidf_transform(corpus, tfidf_params, TFIDF_DIR):\n","#     tfidf = TfidfVectorizer(min_df=5\n","#                             , max_df = 0.3\n","#                             , ngram_range = (1,2)\n","#                             , smooth_idf = True\n","#                            )\n","    # tfidf = TfidfVectorizer(tfidf_params)\n","    # features = tfidf.fit_transform(corpus)\n","    tfidf = pickle.load(open(TFIDF_DIR, 'rb'))\n","    features = tfidf.transform(corpus)\n","    return features"],"execution_count":37,"outputs":[]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR):\n","    tfidf = TfidfVectorizer(tfidf_params)\n","    tfidf = tfidf.fit(code_blocks)\n","    tfidf = pickle.load(open(TFIDF_DIR, 'rb'))\n","    code_blocks_tfidf = tfidf.transform(code_blocks)\n","    return code_blocks_tfidf"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"-clcrT5-jxAl","scrolled":true,"cell_id":"3041592d-1fa0-44c3-93b5-3a0683be87e1"},"source":["def logreg_evaluate(df, code_blocks, TAG_TO_PREDICT):\n","    code_blocks_tfidf = tfidf_fit_transform(code_blocks, tfidf_params, TFIDF_DIR)\n","    X_train, X_test, y_train, y_test = train_test_split(code_blocks_tfidf, df[TAG_TO_PREDICT], test_size=0.25)\n","    clf = LogisticRegression(random_state=421).fit(X_train, y_train)\n","    # clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    accuracy = clf.score(X_test, y_test)\n","    f1 = f1_score(y_pred, y_test)\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","    print(f'F1-score {round(f1*100, 2)}%')\n","    errors = y_test - y_pred\n","    plt.hist(errors)\n","    plot_precision_recall_curve(clf, X_test, y_test)\n","    plot_confusion_matrix(clf, X_test, y_test, values_format='d')\n","    def mean_confidence_interval(data, confidence=0.95):\n","        a = 1.0 * np.array(data)\n","        n = len(a)\n","        m, se = np.mean(a), scipy.stats.sem(a)\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","        return m, m-h, m+h\n","    conf_interval = mean_confidence_interval(errors, 0.95)\n","    print(conf_interval)\n","    metrics = {'test_accuracy': accuracy\n","               , 'test_f1_score': f1}\n","    return metrics"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"42e2ba63-38e5-444d-a9f3-0f26776f3b63"},"source":["def get_predictions(X, y, TAG_TO_PREDICT, MODEL_DIR):\r\n","    clf = pickle.load(open(MODEL_DIR, 'rb'))\r\n","    # result = loaded_model.score(X, y)\r\n","    y_pred = clf.predict(X)\r\n","    accuracy = accuracy_score(y_pred, y)\r\n","    f1 = f1_score(y_pred, y, average='weighted')\r\n","    print(f'Mean Accuracy {round(accuracy*100, 2)}%')\r\n","    print(f'F1-score {round(f1*100, 2)}%')\r\n","    errors = y - y_pred\r\n","    plt.hist(errors)\r\n","    plot_precision_recall_curve(clf, X, y)\r\n","    plot_confusion_matrix(clf, X, y, values_format='d')\r\n","    def mean_confidence_interval(data, confidence=0.95):\r\n","        a = 1.0 * np.array(data)\r\n","        n = len(a)\r\n","        m, se = np.mean(a), scipy.stats.sem(a)\r\n","        h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\r\n","        return m, m-h, m+h\r\n","    conf_interval = mean_confidence_interval(errors, 0.95)\r\n","    print(conf_interval)\r\n","    metrics = {'test_accuracy': accuracy\r\n","               , 'test_f1_score': f1}\r\n","    return metrics"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"d1rIH7pBtjFq","cell_id":"7dc1bcd5-fe3e-49f8-8211-62d7f8ddf438","output_cleared":false,"tags":[]},"source":["if __name__ == '__main__':\n","    DATASET_PATH = './data/code_blocks_regex_graph_v2.csv'\n","    MODEL_DIR = './models/logreg_regex_graph_v2.sav'\n","    TFIDF_DIR = './models/tfidf_logreg.pickle'\n","    CODE_COLUMN = 'code_block'\n","    TAG_TO_PREDICT = 'preprocessing'\n","    df, code_blocks = load_corpus(DATASET_PATH, CODE_COLUMN)\n","    nrows = df.shape[0]\n","    print(\"loaded\")\n","    tfidf_params = {'min_df': 5\n","                    , 'max_df': 0.3\n","                    , 'smooth_idf': True}\n","    data_meta = {'DATASET_PATH': DATASET_PATH\n","                ,'nrows': nrows\n","                ,'label': TAG_TO_PREDICT\n","                ,'model': 'Logistic Regression'}\n","    print(\"tfidf-ed\")\n","    with dagshub.dagshub_logger() as logger:\n","        metrics = logreg_evaluate(df, code_blocks, TAG_TO_PREDICT)\n","        # metrics = get_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\n","        logger.log_hyperparams(data_meta)\n","        logger.log_hyperparams(tfidf_params)\n","        logger.log_metrics(metrics)\n","    print(\"finished\")"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":"index  Unnamed: 0                                         code_block  \\\n0      0           0  \\nimport pandas as pd\\nimport matplotlib.pyplo...   \n1      1           1    \\ndf_train = pd.read_csv('../input/train.csv'),   \n2      2           2                                \\ndf_train.columns,   \n3      3           3                \\ndf_train['SalePrice'].describe(),   \n4      4           4            \\nsns.distplot(df_train['SalePrice']);,   \n\n                                      tag  imports  data_load  preprocessing  \\\n0  ['invite people for the kaggle party']        1          0              0   \n1              ['bring in the six packs']        0          1              0   \n2                ['check the decoration']        0          0              0   \n3      ['descriptive statistics summary']        0          0              0   \n4                           ['histogram']        0          0              0   \n\n   visualization  model  train  predict  \n0              1      0      0        0  \n1              0      0      0        0  \n2              0      0      0        0  \n3              0      0      0        0  \n4              1      0      0        0  \nloaded\ntfidf-ed\n"},{"output_type":"error","ename":"NameError","evalue":"name 'features' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-41-b549ef652804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tfidf-ed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdagshub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdagshub_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTAG_TO_PREDICT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# metrics = get_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-39-ed04599d43dc>\u001b[0m in \u001b[0;36mlogreg_evaluate\u001b[1;34m(df, code_blocks, TAG_TO_PREDICT)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcode_blocks_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_fit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFIDF_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtag_to_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTAG_TO_PREDICT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag_to_predict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag_to_predict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m421\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# clf.fit(X_train, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"]}]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# if __name__ == '__main__':\n","#     all_metrics = []\n","#     chunk_sizes = [5, 10, 15, 20, 25, 30, 40]\n","#     for i in chunk_sizes:\n","#         DATASET_PATH = '/home/jovyan/work/chunks_{}_validate.csv'.format(i)\n","#         MODEL_DIR = '/home/jovyan/work/logreg_regex.sav'\n","#         TFIDF_DIR = '/home/jovyan/work/tfidf.pickle'\n","#         CODE_COLUMN = 'code'\n","#         TAG_TO_PREDICT = 'tag'\n","#         df, corpus = load_corpus(DATASET_PATH, CODE_COLUMN)\n","#         nrows = df.shape[0]\n","#         print(\"loaded\")\n","#         params = {'min_df': 5\n","#                 , 'max_df': 0.3\n","#                 , 'smooth_idf': True}\n","#         data_meta = {'DATASET_PATH': DATASET_PATH\n","#                     ,'nrows': nrows\n","#                     ,'label': TAG_TO_PREDICT\n","#                     ,'model': 'Logistic Regression'}\n","#         features = tfidf_transform(corpus, params, TFIDF_DIR)\n","#         print(\"tfidf-ed\")\n","#         with dagshub.dagshub_logger() as logger:\n","#             # metrics = logreg_evaluate(df, features, TAG_TO_PREDICT)\n","#             metrics = get_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\n","#             all_metrics.append(metrics)\n","#             logger.log_hyperparams(data_meta)\n","#             logger.log_hyperparams(params)\n","#             logger.log_metrics(metrics)\n","#         print(\"finished\")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# all_acc = [all_metrics[i]['test_accuracy'] for i in range(len(all_metrics))]\n","# all_f1 = [all_metrics[i]['test_f1_score'] for i in range(len(all_metrics))]\n","# plt.scatter(chunk_sizes, all_acc)\n","# plt.scatter(chunk_sizes, all_f1)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# def analyze_predictions(X, y, TAG_TO_PREDICT, MODEL_DIR):\n","#     clf = pickle.load(open(MODEL_DIR, 'rb'))\n","#     # result = loaded_model.score(X, y)\n","#     y_pred = clf.predict(X)\n","#     accuracy = accuracy_score(y_pred, y)\n","#     f1 = f1_score(y_pred, y, average='weighted')\n","#     print(f'Mean Accuracy {round(accuracy*100, 2)}%')\n","#     print(f'F1-score {round(f1*100, 2)}%')\n","#     errors = y - y_pred\n","#     plt.hist(errors)\n","#     plot_precision_recall_curve(clf, X, y)\n","#     plot_confusion_matrix(clf, X, y, values_format='d')\n","#     def mean_confidence_interval(data, confidence=0.95):\n","#         a = 1.0 * np.array(data)\n","#         n = len(a)\n","#         m, se = np.mean(a), scipy.stats.sem(a)\n","#         h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n","#         return m, m-h, m+h\n","#     conf_interval = mean_confidence_interval(errors, 0.95)\n","#     print(conf_interval)\n","#     metrics = {'test_accuracy': accuracy\n","#                , 'test_f1_score': f1}\n","#     return X, y, y_pred\n","# if __name__ == '__main__':\n","#     DATASET_PATH = '/home/jovyan/work/chunks_5_validate.csv'\n","#     MODEL_DIR = '/home/jovyan/work/logreg_regex.sav'\n","#     TFIDF_DIR = '/home/jovyan/work/tfidf.pickle'\n","#     CODE_COLUMN = 'code'\n","#     TAG_TO_PREDICT = 'tag'\n","#     df, corpus = load_corpus(DATASET_PATH, CODE_COLUMN)\n","#     nrows = df.shape[0]\n","#     print(\"loaded\")\n","#     params = {'min_df': 5\n","#              , 'max_df': 0.3\n","#              , 'smooth_idf': True}\n","#     data_meta = {'DATASET_PATH': DATASET_PATH\n","#                 ,'nrows': nrows\n","#                 ,'label': TAG_TO_PREDICT\n","#                 ,'model': 'Logistic Regression'}\n","#     features = tfidf_transform(corpus, params, TFIDF_DIR)\n","#     print(\"tfidf-ed\")\n","#     X, y, y_pred = analyze_predictions(features, df[TAG_TO_PREDICT], TAG_TO_PREDICT, MODEL_DIR)\n","#     print(\"finished\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# pd.set_option('max_colwidth', 500)\n","# pd.set_option('max_rows', 500)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# ## False Positives\n","# corpus[(y_pred-y)==-1].reset_index(drop=True)[:]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# from operator import itemgetter\n","# def show_most_informative_features(model, vectorizer=None, text=None, n=20):\n","#     \"\"\"\n","#     Extract the vectorizer and the classifier from the pipeline\n","#     \"\"\"\n","#     if vectorizer is None:\n","#         vectorizer = model.named_steps['vectorizer']\n","#     else:\n","#         text = vectorizer.transform([text])\n","\n","#     classifier = model#.named_steps['classifier']\n","#     feat_names = vectorizer.get_feature_names()\n","\n","#     # Check to make sure that we can perform this computation\n","#     if not hasattr(classifier, 'coef_'):\n","#         raise TypeError(\n","#             \"Cannot compute most informative features on {}.\".format(\n","#                 classifier.__class__.__name__\n","#             )\n","#         )    \n","\n","#     # Otherwise simply use the coefficients\n","#     tvec = classifier.coef_\n","\n","#     # Zip the feature names with the coefs and sort   \n","#     coefs = sorted(\n","#         zip(tvec[0], feat_names),\n","#         key=itemgetter(0), reverse=True\n","#     )\n","\n","#     # Get the top n and bottom n coef, name pairs\n","#     topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n","\n","#     # Create the output string to return\n","#     output = []\n","\n","#     # If text, add the predicted value to the output.\n","#     if text is not None:\n","#         # output.append(\"\\\"{}\\\"\".format(text))\n","#         # output.append(\n","#         #     \"Classified as: {}\".format(model.predict(text))\n","#         # )\n","#         # output.append(\"\")\n","#         print(\"Classified as: {}\".format(model.predict(text)))\n","#     # Create two columns with most negative and most positive features.\n","\n","#     for (cp, fnp), (cn, fnn) in topn:\n","#         print(cp, fnp, cn, fnn)\n","#         # output.append(\n","#         #     \"{:0.2f}{: >15}    {:0.2f}{: >15}\".format(\n","#         #         cp, fnp, cn, fnn\n","#         #     )\n","#         # )\n","\n","#     return \"\\n\".join(output)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# show_most_informative_features(model=pickle.load(open(MODEL_DIR, 'rb')),\n","#                                 vectorizer=pickle.load(open(TFIDF_DIR, 'rb')),\n","#                                 text=corpus[5])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"colab":{"collapsed_sections":[],"name":"NL2ML: LogReg.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.5 32-bit","language":"python","name":"python_defaultSpec_1597164446259"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"deepnote_notebook_id":"2fa6153d-52cd-43ae-af2d-05210d5785e2","deepnote_execution_queue":[]}}