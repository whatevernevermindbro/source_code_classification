{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "NL2ML: BERT Experiments",
      "provenance": [],
      "collapsed_sections": [
        "8Qul1odRtje9",
        "Vc8DgS51xR22",
        "h7BABWRdV7KG",
        "oLSKZ-RyV7KK",
        "2oi3xIwRV7KS",
        "uagdJ4dzpq7m"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i2PK_6ES7ysS",
        "colab_type": "code",
        "outputId": "6a38c7f3-3836-4152-99ff-8d2d966ad151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow_hub==0.7\n",
        "!pip install tensorflow==2.0.0\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_hub==0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.7) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.7) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.7) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub==0.7) (46.1.3)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Found existing installation: tensorflow-hub 0.8.0\n",
            "    Uninstalling tensorflow-hub-0.8.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.8.0\n",
            "Successfully installed tensorflow-hub-0.7.0\n",
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 74kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.28.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f769b0d39f8ceba2b03d66f7e4f07da8a0a046604c73c1e6fdda5b6de741da26\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n",
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/84/1bea6c34d38f3e726830d3adeca76e6e901b98cf5babd635883dbedd7ecc/bert-for-tf2-0.14.1.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 1.9MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/0d/615c0d4aea541b4f47c761263809a02e160e7a2babd175f0ddd804776cf4/params-flow-0.8.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.38.0)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.1-cp36-none-any.whl size=30083 sha256=ce2cb04f9108e7dd558d1932f1324307b259f132b9c0b45b682bee2b74974048\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/f1/10/861fd7899727e4034293fb1dfef45b00f8cd476d21d3b3821e\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=5f77ef47ba8a27bcf695f11668b945f9358680ac68c41887a5888840f55a376f\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.0-cp36-none-any.whl size=15999 sha256=621e9f8a6f0de476ed9103fe8926c22f78fc8488764be0f91c690b58d5de4543\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/41/05/1a9955d1d01575bbd58aab76e22f8c7eeabba905d551576f43\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.1 params-flow-0.8.0 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jinUj9ik7yse",
        "colab_type": "code",
        "outputId": "d1a42a70-4b10-4740-dc05-c1b2c02a599a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version:  2.0.0\n",
            "Hub version:  0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o8xxNpou7ysm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import bert\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import wasserstein_distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqjQDMKp3P9",
        "colab_type": "text"
      },
      "source": [
        "### Preparing BERT and Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "as1Cu7Th7yst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 512  # Your choice here.\n",
        "\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gQ5Arwvl7ys0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "\n",
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ACmlB2yr7ys_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_sentence(sentence):\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "    \n",
        "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
        "    input_masks = get_masks(stokens, max_seq_length)\n",
        "    input_segments = get_segments(stokens, max_seq_length)\n",
        "    \n",
        "    return input_ids, input_masks, input_segments\n",
        "\n",
        "def compare_sentences(sentence_1, sentence_2, distance_metric):\n",
        "    input_ids_1, input_masks_1, input_segments_1 = tokenize_sentence(sentence_1)\n",
        "    input_ids_2, input_masks_2, input_segments_2 = tokenize_sentence(sentence_2)\n",
        "    \n",
        "    pool_embs_1, all_embs_1 = model.predict([[input_ids_1],[input_masks_1],[input_segments_1]])\n",
        "    pool_embs_2, all_embs_2 = model.predict([[input_ids_2],[input_masks_2],[input_segments_2]])\n",
        "#     print(pool_embs_1, all_embs_1)\n",
        "#     print(pool_embs_2, all_embs_2)\n",
        "    return distance_metric(pool_embs_1[0], pool_embs_2[0])\n",
        "\n",
        "def distance_between_sentences(sentence_1, sentence_2, distance_metric):\n",
        "    input_ids_1, input_masks_1, input_segments_1 = tokenize_sentence(sentence_1)\n",
        "    input_ids_2, input_masks_2, input_segments_2 = tokenize_sentence(sentence_2)\n",
        "    pool_embs_1, all_embs_1 = model.predict([[input_ids_1],[input_masks_1],[input_segments_1]])\n",
        "    pool_embs_2, all_embs_2 = model.predict([[input_ids_2],[input_masks_2],[input_segments_2]])\n",
        "    distances = []\n",
        "    for i in range(0,max_seq_length):\n",
        "      distances.append(distance_metric(all_embs_1[0][i], all_embs_2[0][i]))\n",
        "    distance = np.mean(distances)\n",
        "    return distance\n",
        "\n",
        "def get_embs(sentence):\n",
        "    input_ids, input_masks, input_segments = tokenize_sentence(sentence)\n",
        "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "    return pool_embs, all_embs\n",
        "\n",
        "def get_all_embs(sentence):\n",
        "    print(sentence)\n",
        "    input_ids, input_masks, input_segments = tokenize_sentence(sentence)\n",
        "    _, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "    return all_embs\n",
        "\n",
        "def square_rooted(x):\n",
        "    return math.sqrt(sum([a*a for a in x]))\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return 1-numerator/float(denominator)\n",
        "\n",
        "def dummy_metric(x,y):\n",
        "    return 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qul1odRtje9",
        "colab_type": "text"
      },
      "source": [
        "## Distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc8DgS51xR22",
        "colab_type": "text"
      },
      "source": [
        "### Natural Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8w0XXYDw0ty",
        "colab_type": "code",
        "outputId": "846f2576-d80c-4f46-90f0-bab4dd24a9df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "s = []\n",
        "s.append('How are you doing?')\n",
        "s.append('How much are we feeling?')\n",
        "s.append('What are you doing?')\n",
        "s.append('What`s up?')\n",
        "s.append('Are you doing?')\n",
        "central = s[0]\n",
        "print(\"Central phrase: '{}'\".format(central))\n",
        "for sentence in s:\n",
        "    print(\"Cosine Distance to '{}' = {}\".format(sentence, round(distance_between_sentences(central, sentence, cosine_similarity), 3)))\n",
        "    print(\"Wasserstein Distance to '{}' = {}\".format(sentence, round(distance_between_sentences(central, sentence, wasserstein_distance), 3)))\n",
        "    print(\"-----------------\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Central phrase: 'How are you doing?'\n",
            "Cosine Distance to 'How are you doing?' = -0.0\n",
            "Wasserstein Distance to 'How are you doing?' = 0.0\n",
            "-----------------\n",
            "Cosine Distance to 'How much are we feeling?' = 0.292\n",
            "Wasserstein Distance to 'How much are we feeling?' = 0.066\n",
            "-----------------\n",
            "Cosine Distance to 'What are you doing?' = 0.162\n",
            "Wasserstein Distance to 'What are you doing?' = 0.015\n",
            "-----------------\n",
            "Cosine Distance to 'What`s up?' = 0.275\n",
            "Wasserstein Distance to 'What`s up?' = 0.047\n",
            "-----------------\n",
            "Cosine Distance to 'Are you doing?' = 0.244\n",
            "Wasserstein Distance to 'Are you doing?' = 0.072\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7BABWRdV7KG",
        "colab_type": "text"
      },
      "source": [
        "## Source Code Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_vIcIgNV7KG",
        "colab_type": "code",
        "outputId": "ba195daf-8b6f-425a-fd43-3411e05687d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "w = []\n",
        "w.append('data')\n",
        "w.append('Data')\n",
        "w.append('Data Set')\n",
        "w.append('datadata')\n",
        "w.append('dataset')\n",
        "w.append('DataFrame')\n",
        "w.append('dataframe')\n",
        "w.append('df')\n",
        "w.append('pd.DataFrame')\n",
        "central = w[6]\n",
        "print(\"Central phrase: '{}'\".format(central))\n",
        "for word in w:\n",
        "    print(\"Cosine Distance to '{}' = {}\".format(word, round(distance_between_sentences(central, word, cosine_similarity), 3)))\n",
        "    print(\"Wasserstein Distance to '{}' = {}\".format(word, round(distance_between_sentences(central, word, wasserstein_distance), 3)))\n",
        "    print(\"-----------------\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Central phrase: 'dataframe'\n",
            "Cosine Distance to 'data' = 0.214\n",
            "Wasserstein Distance to 'data' = 0.035\n",
            "-----------------\n",
            "Cosine Distance to 'Data' = 0.214\n",
            "Wasserstein Distance to 'Data' = 0.035\n",
            "-----------------\n",
            "Cosine Distance to 'Data Set' = 0.19\n",
            "Wasserstein Distance to 'Data Set' = 0.013\n",
            "-----------------\n",
            "Cosine Distance to 'datadata' = 0.218\n",
            "Wasserstein Distance to 'datadata' = 0.041\n",
            "-----------------\n",
            "Cosine Distance to 'dataset' = 0.141\n",
            "Wasserstein Distance to 'dataset' = 0.02\n",
            "-----------------\n",
            "Cosine Distance to 'DataFrame' = -0.0\n",
            "Wasserstein Distance to 'DataFrame' = 0.0\n",
            "-----------------\n",
            "Cosine Distance to 'dataframe' = -0.0\n",
            "Wasserstein Distance to 'dataframe' = 0.0\n",
            "-----------------\n",
            "Cosine Distance to 'df' = 0.403\n",
            "Wasserstein Distance to 'df' = 0.073\n",
            "-----------------\n",
            "Cosine Distance to 'pd.DataFrame' = 0.186\n",
            "Wasserstein Distance to 'pd.DataFrame' = 0.071\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLSKZ-RyV7KK",
        "colab_type": "text"
      },
      "source": [
        "## Source Code Chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar1Elwc_V7KN",
        "colab_type": "code",
        "outputId": "44893c37-a73b-4304-ed23-e6dbe3570fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "c = []\n",
        "cnot = []\n",
        "c.append(\"\"\"\n",
        "def apply_window(image, center, width):\n",
        "    image = image.copy()\n",
        "\n",
        "    min_value = center - width // 2\n",
        "    max_value = center + width // 2\n",
        "\n",
        "    image[image < min_value] = min_value\n",
        "    image[image > max_value] = max_value\n",
        "\n",
        "    return image\n",
        "\"\"\")\n",
        "c.append(\"\"\"\n",
        "def image_windowed(image, custom_center=50, custom_width=130, out_side_val=False):\n",
        "    '''\n",
        "    Important thing to note in this function: The image migth be changed in place!\n",
        "    '''\n",
        "    # see: https://www.kaggle.com/allunia/rsna-ih-detection-eda-baseline\n",
        "    min_value = custom_center - (custom_width/2)\n",
        "    max_value = custom_center + (custom_width/2)\n",
        "    \n",
        "    # Including another value for values way outside the range, to (hopefully) make segmentation processes easier. \n",
        "    out_value_min = custom_center - custom_width\n",
        "    out_value_max = custom_center + custom_width\n",
        "    \n",
        "    if out_side_val:\n",
        "        image[np.logical_and(image < min_value, image > out_value_min)] = min_value\n",
        "        image[np.logical_and(image > max_value, image < out_value_max)] = max_value\n",
        "        image[image < out_value_min] = out_value_min\n",
        "        image[image > out_value_max] = out_value_max\n",
        "    \n",
        "    else:\n",
        "        image[image < min_value] = min_value\n",
        "        image[image > max_value] = max_value\n",
        "    \n",
        "    return image\n",
        "\"\"\")\n",
        "c.append(\"\"\"\n",
        "def image_crop(image):\n",
        "    # Based on this stack overflow post: https://stackoverflow.com/questions/26310873/how-do-i-crop-an-image-on-a-white-background-with-python\n",
        "    mask = image == 0\n",
        "\n",
        "    # Find the bounding box of those pixels\n",
        "    coords = np.array(np.nonzero(~mask))\n",
        "    top_left = np.min(coords, axis=1)\n",
        "    bottom_right = np.max(coords, axis=1)\n",
        "\n",
        "    out = image[top_left[0]:bottom_right[0],\n",
        "                top_left[1]:bottom_right[1]]\n",
        "    \n",
        "    return out\n",
        "\"\"\")\n",
        "cnot.append(\"\"\"\n",
        "def normalize_minmax(img):\n",
        "    mi, ma = img.min(), img.max()\n",
        "    return (img - mi) / (ma - mi)\"\"\")\n",
        "cnot.append(\"\"\"def normalize(img, means, stds, tensor=False):\n",
        "    return (img - means)/stds\"\"\")\n",
        "cnot.append(\"\"\"X_train = X_train / 255.0\n",
        "test = test / 255.0\n",
        "mean_px = X_train.mean().astype(np.float32)\n",
        "std_px = X_train.std().astype(np.float32)\"\"\")\n",
        "cnot.append(\"\"\"def standardize(x): \n",
        "    return (x-mean_px)/std_px\"\"\")\n",
        "cnot.append(\"\"\"def rle_decode(mask_rle, shape=(768, 768)):\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape).T  # Needed to align to RLE direction\"\"\")\n",
        "cnot.append(\"\"\"def mask2rle(img):\n",
        "    pixels = img.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\"\"\")\n",
        "cnot.append(\"\"\"def deskew(img):\n",
        "    m = cv2.moments(img)\n",
        "    if abs(m['mu02']) < 1e-2:\n",
        "        return img.copy()\n",
        "    skew = m['mu11']/m['mu02']\n",
        "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
        "    img = cv2.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
        "    return img\"\"\")\n",
        "cnot.append(\"\"\"image = imageio.imread('../input/image1.jpg')\n",
        "image_rotated = rotate.augment_images([image])\n",
        "image_noise = gaussian_noise.augment_images([image])\n",
        "image_crop = crop.augment_images([image])\n",
        "image_hue = hue.augment_images([image])\n",
        "image_trans = elastic_trans.augment_images([image])\n",
        "image_coarse = coarse_drop.augment_images([image])\"\"\")\n",
        "\n",
        "central = c[0]\n",
        "results_c = []\n",
        "results_c_wasserstein = []\n",
        "for chunk in c:\n",
        "    results_c.append(distance_between_sentences(central, chunk, cosine_similarity))\n",
        "    results_c_wasserstein.append(distance_between_sentences(central, chunk, wasserstein_distance))\n",
        "\n",
        "results_cnot = []\n",
        "results_cnot_wasserstein = []\n",
        "for chunk in cnot:\n",
        "    results_cnot.append(distance_between_sentences(central, chunk, cosine_similarity))\n",
        "    results_cnot_wasserstein.append(distance_between_sentences(central, chunk, wasserstein_distance))\n",
        "    \n",
        "print(np.mean(results_c), np.mean(results_cnot))\n",
        "print(np.mean(results_c_wasserstein), np.mean(results_cnot_wasserstein))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.28992930316407933 0.34256187616174183\n",
            "0.06115377509762359 0.05829068909336778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oi3xIwRV7KS",
        "colab_type": "text"
      },
      "source": [
        "## NL2ML Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds3PdEazV7KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nl2ml = pd.read_csv('/content/nl2ml_images.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzr3ZCz-V7Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_names = nl2ml['Preprocessing class (for doc about methods)'].value_counts().head(5).keys().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RukTdQJTV7Ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3cbb3131-7954-4efd-8a81-efa826e97866"
      },
      "source": [
        "central = nl2ml[(nl2ml['Preprocessing class (for doc about methods)'] == cat_names[1])]['Code'].reset_index(drop=True)[0]\n",
        "results = {\"central\":cat_names[0]}\n",
        "for cat in cat_names:\n",
        "    print(cat)\n",
        "    rows = nl2ml[(nl2ml['Preprocessing class (for doc about methods)'] == cat)].reset_index(drop=True)\n",
        "    chunks = rows['Code']\n",
        "    results_c = []\n",
        "    for chunk in chunks:\n",
        "        if len(chunk) <= 512:\n",
        "            results_c.append(distance_between_sentences(central, chunk, wasserstein_distance))\n",
        "        else: continue\n",
        "    results.update({cat:round(np.mean(results_c), 3)})\n",
        "    del results_c\n",
        "print(results)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting annotations\n",
            "Visualization\n",
            "Augmentation\n",
            "Converting (coordinates)\n",
            "Resizing\n",
            "{'central': 'Getting annotations', 'Getting annotations': 0.069, 'Visualization': 0.062, 'Augmentation': 0.077, 'Converting (coordinates)': 0.062, 'Resizing': 0.074}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izdGeYHu2_VE",
        "colab_type": "code",
        "outputId": "e15ca722-cd75-449c-8a90-52126859a50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "results"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Augmentation': 0.077,\n",
              " 'Converting (coordinates)': 0.062,\n",
              " 'Getting annotations': 0.069,\n",
              " 'Resizing': 0.074,\n",
              " 'Visualization': 0.062,\n",
              " 'central': 'Getting annotations'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOrE2VurBvs",
        "colab_type": "text"
      },
      "source": [
        "!!! Very little amount of data used here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uagdJ4dzpq7m",
        "colab_type": "text"
      },
      "source": [
        "## Chunks into Embeddings\n",
        "Use __get_embs(chunk)__ to get embedding for a chunk\n",
        "\n",
        "Use __distance_between_sentences(chunk_1, chunk_2, distance_metric)__ to get distance between two chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEYcvO6ZqUlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc_chunk_1 = \"\"\"\n",
        "def apply_window(image, center, width):\n",
        "    image = image.copy()\n",
        "\n",
        "    min_value = center - width // 2\n",
        "    max_value = center + width // 2\n",
        "\n",
        "    image[image < min_value] = min_value\n",
        "    image[image > max_value] = max_value\n",
        "\n",
        "    return image\n",
        "\"\"\"\n",
        "sc_chunk_2 = \"\"\"\n",
        "def normalize_minmax(img):\n",
        "    mi, ma = img.min(), img.max()\n",
        "    return (img - mi) / (ma - mi)\n",
        "\"\"\"\n",
        "sc_chunk_3 = \"\"\"\n",
        "def lalalalallalalala:\n",
        "  np.argmax([1,2,3,4,5,6,7,8,9,10])\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDunlg5wppJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pool_embs, all_embs = get_embs(sc_chunk_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gc8vCEWqZFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de315399-4d84-4a49-ccb0-d9938d6b5d0e"
      },
      "source": [
        "print(distance_between_sentences(sc_chunk_1, sc_chunk_2, cosine_similarity))\n",
        "print(distance_between_sentences(sc_chunk_1, sc_chunk_2, wasserstein_distance))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.33366158501913135\n",
            "0.05372519776344532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipyfdu4xx3K_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "362e6b12-2ef9-4df5-b7ff-6304143ae656"
      },
      "source": [
        "print(distance_between_sentences(sc_chunk_2, sc_chunk_3, cosine_similarity))\n",
        "print(distance_between_sentences(sc_chunk_2, sc_chunk_3, wasserstein_distance))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4874605344472024\n",
            "0.055971320470390606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nLGMMK5x4Mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "84708a22-9089-4b61-e38b-849db95245be"
      },
      "source": [
        "print(distance_between_sentences(sc_chunk_1, sc_chunk_3, cosine_similarity))\n",
        "print(distance_between_sentences(sc_chunk_1, sc_chunk_3, wasserstein_distance))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5361630921223379\n",
            "0.06683835241049466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvfOR5Qr3yDZ",
        "colab_type": "text"
      },
      "source": [
        "## Getting the Embeddings for the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPt9Qg5ZdX_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "7357e706-ef0e-42ba-e7f7-740ef20d4547"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_CklmUL35at",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KK_PATH = '/content/drive/My Drive/NL2ML/Tigran_parser/kaggle_kernels/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNMxMypsx63n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4d6ade7c-5c6b-4ba5-f4e2-f09919ab144b"
      },
      "source": [
        "df = pd.read_csv(KK_PATH + 'code_blocks.csv', sep='\\t')\n",
        "df.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149170, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sKy1VzK4Q4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-gH10JdEum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwr-bHIcdsj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQGNFVbkEkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['code_block_length'] = np.zeros(len(df))\n",
        "for i in range(len(df)):\n",
        "  print(str(i)+'\\r', end='')\n",
        "  df['code_block_length'][i] = len(df['code_block'][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfh16EYdo88B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(df[df['code_block_length'] > 512].index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs3dH9fDdiu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "daa6e1e6-d0ee-4443-f023-35bd56bba0c8"
      },
      "source": [
        "df['embeddings'] = np.zeros(df.shape[0])\n",
        "# df['embeddings'][0:10] = df['code_block'][0:10].apply(get_all_embs) ## Check\n",
        "df['embeddings'] = df['code_block'].apply(get_all_embs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#invite people for the Kaggle party\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "import numpy as np\n",
            "from scipy.stats import norm\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from scipy import stats\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n",
            "#bring in the six packs\n",
            "#check the decoration\n",
            "#descriptive statistics summary\n",
            "\n",
            "#histogram\n",
            "#skewness and kurtosis\n",
            "print('Skewness: %f' % df_train['SalePrice'].skew())\n",
            "#scatter plot grlivarea/saleprice\n",
            "var = 'GrLivArea'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "#scatter plot totalbsmtsf/saleprice\n",
            "var = 'TotalBsmtSF'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "#box plot overallqual/saleprice\n",
            "var = 'OverallQual'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "f, ax = plt.subplots(figsize=(8, 6))\n",
            "fig = sns.boxplot(x=var, y='SalePrice', data=data)\n",
            "var = 'YearBuilt'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "f, ax = plt.subplots(figsize=(16, 8))\n",
            "fig = sns.boxplot(x=var, y='SalePrice', data=data)\n",
            "fig.axis(ymin=0, ymax=800000);\n",
            "#correlation matrix\n",
            "corrmat = df_train.corr()\n",
            "f, ax = plt.subplots(figsize=(12, 9))\n",
            "#saleprice correlation matrix\n",
            "k = 10 #number of variables for heatmap\n",
            "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
            "cm = np.corrcoef(df_train[cols].values.T)\n",
            "sns.set(font_scale=1.25)\n",
            "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
            "#scatterplot\n",
            "sns.set()\n",
            "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
            "sns.pairplot(df_train[cols], size = 2.5)\n",
            "#missing data\n",
            "total = df_train.isnull().sum().sort_values(ascending=False)\n",
            "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
            "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
            "#dealing with missing data\n",
            "df_train = df_train.drop((missing_data[missing_data['Total']  1]).index,1)\n",
            "df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\n",
            "#standardizing data\n",
            "saleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\n",
            "low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n",
            "high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n",
            "print('outer range (low) of the distribution:')\n",
            "print(low_range)\n",
            "print('\n",
            "outer range (high) of the distribution:')\n",
            "#bivariate analysis saleprice/grlivarea\n",
            "var = 'GrLivArea'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "#deleting points\n",
            "df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
            "df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\n",
            "#bivariate analysis saleprice/grlivarea\n",
            "var = 'TotalBsmtSF'\n",
            "data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n",
            "#histogram and normal probability plot\n",
            "sns.distplot(df_train['SalePrice'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#applying log transformation\n",
            "#transformed histogram and normal probability plot\n",
            "sns.distplot(df_train['SalePrice'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#histogram and normal probability plot\n",
            "sns.distplot(df_train['GrLivArea'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#data transformation\n",
            "#transformed histogram and normal probability plot\n",
            "sns.distplot(df_train['GrLivArea'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#histogram and normal probability plot\n",
            "sns.distplot(df_train['TotalBsmtSF'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#create column for new variable (one is enough because it's a binary categorical feature)\n",
            "#if area0 it gets 1, for area==0 it gets 0\n",
            "df_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\n",
            "df_train['HasBsmt'] = 0 \n",
            "#transform data\n",
            "#histogram and normal probability plot\n",
            "sns.distplot(df_train[df_train['TotalBsmtSF']0]['TotalBsmtSF'], fit=norm);\n",
            "fig = plt.figure()\n",
            "#scatter plot\n",
            "#scatter plot\n",
            "#convert categorical variable into dummy\n",
            "train_df = pd.read_csv('../input/train.csv')\n",
            "test_df = pd.read_csv('../input/test.csv')\n",
            "# preview the data\n",
            "train_df.info()\n",
            "print('_'*40)\n",
            "train_df.describe()\n",
            "# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n",
            "# Review Parch distribution using `percentiles=[.75, .8]`\n",
            "# SibSp distribution `[.68, .69]`\n",
            "g = sns.FacetGrid(train_df, col='Survived')\n",
            "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
            "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
            "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
            "# grid = sns.FacetGrid(train_df, col='Embarked')\n",
            "grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\n",
            "grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\n",
            "# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\n",
            "grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n",
            "grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\n",
            "print('Before', train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n",
            "\n",
            "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
            "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
            "combine = [train_df, test_df]\n",
            "\n",
            "for dataset in combine:\n",
            "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+).', expand=False)\n",
            "\n",
            "for dataset in combine:\n",
            "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n",
            " t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
            "\n",
            "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
            "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
            "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
            "    \n",
            "title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
            "for dataset in combine:\n",
            "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
            "    dataset['Title'] = dataset['Title'].fillna(0)\n",
            "\n",
            "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
            "test_df = test_df.drop(['Name'], axis=1)\n",
            "combine = [train_df, test_df]\n",
            "for dataset in combine:\n",
            "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
            "\n",
            "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\n",
            "grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\n",
            "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
            "guess_ages = np.zeros((2,3))\n",
            "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n",
            "for dataset in combine:    \n",
            "    dataset.loc[ dataset['Age'] = 16, 'Age'] = 0\n",
            "    dataset.loc[(dataset['Age']  16)  (dataset['Age'] = 32), 'Age'] = 1\n",
            "    dataset.loc[(dataset['Age']  32)  (dataset['Age'] = 48), 'Age'] = 2\n",
            "    dataset.loc[(dataset['Age']  48)  (dataset['Age'] = 64), 'Age'] = 3\n",
            "    dataset.loc[ dataset['Age']  64, 'Age']\n",
            "train_df = train_df.drop(['AgeBand'], axis=1)\n",
            "combine = [train_df, test_df]\n",
            "for dataset in combine:\n",
            "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
            "\n",
            "for dataset in combine:\n",
            "    dataset['IsAlone'] = 0\n",
            "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
            "\n",
            "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
            "test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
            "combine = [train_df, test_df]\n",
            "\n",
            "for dataset in combine:\n",
            "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n",
            "\n",
            "freq_port = train_df.Embarked.dropna().mode()[0]\n",
            "for dataset in combine:\n",
            "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
            "    \n",
            "for dataset in combine:\n",
            "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
            "\n",
            "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
            "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n",
            "for dataset in combine:\n",
            "    dataset.loc[ dataset['Fare'] = 7.91, 'Fare'] = 0\n",
            "    dataset.loc[(dataset['Fare']  7.91)  (dataset['Fare'] = 14.454), 'Fare'] = 1\n",
            "    dataset.loc[(dataset['Fare']  14.454)  (dataset['Fare'] = 31), 'Fare']   = 2\n",
            "    dataset.loc[ dataset['Fare']  31, 'Fare'] = 3\n",
            "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
            "\n",
            "train_df = train_df.drop(['FareBand'], axis=1)\n",
            "combine = [train_df, test_df]\n",
            "    \n",
            "X_train = train_df.drop('Survived', axis=1)\n",
            "Y_train = train_df['Survived']\n",
            "X_test  = test_df.drop('PassengerId', axis=1).copy()\n",
            "# Logistic Regression\n",
            "\n",
            "logreg = LogisticRegression()\n",
            "logreg.fit(X_train, Y_train)\n",
            "Y_pred = logreg.predict(X_test)\n",
            "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
            "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
            "coeff_df.columns = ['Feature']\n",
            "coeff_df['Correlation'] = pd.Series(logreg.coef_[0])\n",
            "\n",
            "# Support Vector Machines\n",
            "\n",
            "svc = SVC()\n",
            "svc.fit(X_train, Y_train)\n",
            "Y_pred = svc.predict(X_test)\n",
            "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
            "knn = KNeighborsClassifier(n_neighbors = 3)\n",
            "knn.fit(X_train, Y_train)\n",
            "Y_pred = knn.predict(X_test)\n",
            "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
            "# Gaussian Naive Bayes\n",
            "\n",
            "gaussian = GaussianNB()\n",
            "gaussian.fit(X_train, Y_train)\n",
            "Y_pred = gaussian.predict(X_test)\n",
            "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
            "# Perceptron\n",
            "\n",
            "perceptron = Perceptron()\n",
            "perceptron.fit(X_train, Y_train)\n",
            "Y_pred = perceptron.predict(X_test)\n",
            "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
            "# Linear SVC\n",
            "\n",
            "linear_svc = LinearSVC()\n",
            "linear_svc.fit(X_train, Y_train)\n",
            "Y_pred = linear_svc.predict(X_test)\n",
            "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
            "# Stochastic Gradient Descent\n",
            "\n",
            "sgd = SGDClassifier()\n",
            "sgd.fit(X_train, Y_train)\n",
            "Y_pred = sgd.predict(X_test)\n",
            "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
            "# Decision Tree\n",
            "\n",
            "decision_tree = DecisionTreeClassifier()\n",
            "decision_tree.fit(X_train, Y_train)\n",
            "Y_pred = decision_tree.predict(X_test)\n",
            "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
            "# Random Forest\n",
            "\n",
            "random_forest = RandomForestClassifier(n_estimators=100)\n",
            "random_forest.fit(X_train, Y_train)\n",
            "Y_pred = random_forest.predict(X_test)\n",
            "random_forest.score(X_train, Y_train)\n",
            "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
            "models = pd.DataFrame({\n",
            "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
            "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
            "              'Stochastic Gradient Decent', 'Linear SVC', \n",
            "              'Decision Tree'],\n",
            "    'Score': [acc_svc, acc_knn, acc_log, \n",
            "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
            "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
            "submission = pd.DataFrame({\n",
            "        'PassengerId': test_df['PassengerId'],\n",
            "        'Survived': Y_pred\n",
            "    })\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3-EnWnAqqcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}