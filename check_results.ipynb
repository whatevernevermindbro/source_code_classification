{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/github_chunks_10_preprocessing_logreg_v3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "github = pd.read_csv('./data/github_chunks_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(step + 1) % display_step == 0 or step == 0:\n        if step > 0:\n            average_loss /= display_step\n            average_acc /= display_step\n        print(\"Step:\", '%04d' % (step + 1), \" loss=\",\n              \"{:.9f}\".format(average_loss), \" accuracy=\",\n              \"{:.4f}\".format(average_acc))\n        average_loss = 0.\n        average_acc = 0.\ntestX = mnist.test.images\ntestY = mnist.test.labels\n\ntest_acc = accuracy_fn(logistic_regression, testX, testY)\nprint(\"Testset Accuracy: {:.4f}\".format(test_acc))\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\nXtr, Ytr = mnist.train.next_batch(5000) Xte, Yte = mnist.test.next_batch(200) \nxtr = tf.placeholder(\"float\", [None, 784])\nxte = tf.placeholder(\"float\", [784])\n\ndistance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\npred = tf.argmin(distance, 0)\n\naccuracy = 0.\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n\n        for i in range(len(Xte)):\n                nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n                print \"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]),            \"True Class:\", np.argmax(Yte[i])\n                if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n            accuracy += 1./len(Xte)\n================\n        average_acc = 0.\ntestX = mnist.test.images\ntestY = mnist.test.labels\n\ntest_acc = accuracy_fn(logistic_regression, testX, testY)\nprint(\"Testset Accuracy: {:.4f}\".format(test_acc))\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\nXtr, Ytr = mnist.train.next_batch(5000) Xte, Yte = mnist.test.next_batch(200) \nxtr = tf.placeholder(\"float\", [None, 784])\nxte = tf.placeholder(\"float\", [784])\n\ndistance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\npred = tf.argmin(distance, 0)\n\naccuracy = 0.\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n\n        for i in range(len(Xte)):\n                nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n                print \"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]),            \"True Class:\", np.argmax(Yte[i])\n                if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n            accuracy += 1./len(Xte)\n    print \"Done!\"\n    print \"Accuracy:\", accuracy\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import resources\nfrom tensorflow.contrib.tensor_forest.python import tensor_forest\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n================\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\nXtr, Ytr = mnist.train.next_batch(5000) Xte, Yte = mnist.test.next_batch(200) \nxtr = tf.placeholder(\"float\", [None, 784])\nxte = tf.placeholder(\"float\", [784])\n\ndistance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\npred = tf.argmin(distance, 0)\n\naccuracy = 0.\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n\n        for i in range(len(Xte)):\n                nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n                print \"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]),            \"True Class:\", np.argmax(Yte[i])\n                if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n            accuracy += 1./len(Xte)\n    print \"Done!\"\n    print \"Accuracy:\", accuracy\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import resources\nfrom tensorflow.contrib.tensor_forest.python import tensor_forest\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\nnum_steps = 500 batch_size = 1024 num_classes = 10 num_features = 784 num_trees = 10\nmax_nodes = 1000\n\nX = tf.placeholder(tf.float32, shape=[None, num_features])\nY = tf.placeholder(tf.int32, shape=[None])\n\nhparams = tensor_forest.ForestHParams(num_classes=num_classes,\n                                      num_features=num_features,\n================\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n\n        for i in range(len(Xte)):\n                nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n                print \"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]),            \"True Class:\", np.argmax(Yte[i])\n                if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n            accuracy += 1./len(Xte)\n    print \"Done!\"\n    print \"Accuracy:\", accuracy\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import resources\nfrom tensorflow.contrib.tensor_forest.python import tensor_forest\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\nnum_steps = 500 batch_size = 1024 num_classes = 10 num_features = 784 num_trees = 10\nmax_nodes = 1000\n\nX = tf.placeholder(tf.float32, shape=[None, num_features])\nY = tf.placeholder(tf.int32, shape=[None])\n\nhparams = tensor_forest.ForestHParams(num_classes=num_classes,\n                                      num_features=num_features,\n                                      num_trees=num_trees,\n                                      max_nodes=max_nodes).fill()\nforest_graph = tensor_forest.RandomForestGraphs(hparams)\ntrain_op = forest_graph.training_graph(X, Y)\nloss_op = forest_graph.training_loss(X, Y)\n\ninfer_op, _, _ = forest_graph.inference_graph(X)\ncorrect_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\naccuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n================\n    print \"Accuracy:\", accuracy\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import resources\nfrom tensorflow.contrib.tensor_forest.python import tensor_forest\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\nnum_steps = 500 batch_size = 1024 num_classes = 10 num_features = 784 num_trees = 10\nmax_nodes = 1000\n\nX = tf.placeholder(tf.float32, shape=[None, num_features])\nY = tf.placeholder(tf.int32, shape=[None])\n\nhparams = tensor_forest.ForestHParams(num_classes=num_classes,\n                                      num_features=num_features,\n                                      num_trees=num_trees,\n                                      max_nodes=max_nodes).fill()\nforest_graph = tensor_forest.RandomForestGraphs(hparams)\ntrain_op = forest_graph.training_graph(X, Y)\nloss_op = forest_graph.training_loss(X, Y)\n\ninfer_op, _, _ = forest_graph.inference_graph(X)\ncorrect_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\naccuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit_vars = tf.group(tf.global_variables_initializer(),\n    resources.initialize_resources(resources.shared_resources()))\nsess = tf.train.MonitoredSession()\n\nsess.run(init_vars)\n\nfor i in range(1, num_steps + 1):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n    if i % 50 == 0 or i == 1:\n================\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\nnum_steps = 500 batch_size = 1024 num_classes = 10 num_features = 784 num_trees = 10\nmax_nodes = 1000\n\nX = tf.placeholder(tf.float32, shape=[None, num_features])\nY = tf.placeholder(tf.int32, shape=[None])\n\nhparams = tensor_forest.ForestHParams(num_classes=num_classes,\n                                      num_features=num_features,\n                                      num_trees=num_trees,\n                                      max_nodes=max_nodes).fill()\nforest_graph = tensor_forest.RandomForestGraphs(hparams)\ntrain_op = forest_graph.training_graph(X, Y)\nloss_op = forest_graph.training_loss(X, Y)\n\ninfer_op, _, _ = forest_graph.inference_graph(X)\ncorrect_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\naccuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit_vars = tf.group(tf.global_variables_initializer(),\n    resources.initialize_resources(resources.shared_resources()))\nsess = tf.train.MonitoredSession()\n\nsess.run(init_vars)\n\nfor i in range(1, num_steps + 1):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n    if i % 50 == 0 or i == 1:\n        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n\ntest_x, test_y = mnist.test.images, mnist.test.labels\nprint(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))\nfrom __future__ import division, print_function, absolute_import\n\nimport collections\nimport os\nimport random\n================\n                                      max_nodes=max_nodes).fill()\nforest_graph = tensor_forest.RandomForestGraphs(hparams)\ntrain_op = forest_graph.training_graph(X, Y)\nloss_op = forest_graph.training_loss(X, Y)\n\ninfer_op, _, _ = forest_graph.inference_graph(X)\ncorrect_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\naccuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit_vars = tf.group(tf.global_variables_initializer(),\n    resources.initialize_resources(resources.shared_resources()))\nsess = tf.train.MonitoredSession()\n\nsess.run(init_vars)\n\nfor i in range(1, num_steps + 1):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n    if i % 50 == 0 or i == 1:\n        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n\ntest_x, test_y = mnist.test.images, mnist.test.labels\nprint(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))\nfrom __future__ import division, print_function, absolute_import\n\nimport collections\nimport os\nimport random\nimport urllib\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\nlearning_rate = 0.1\nbatch_size = 128\nnum_steps = 3000000\ndisplay_step = 10000\neval_step = 200000\n================\n    resources.initialize_resources(resources.shared_resources()))\nsess = tf.train.MonitoredSession()\n\nsess.run(init_vars)\n\nfor i in range(1, num_steps + 1):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n    if i % 50 == 0 or i == 1:\n        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n\ntest_x, test_y = mnist.test.images, mnist.test.labels\nprint(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))\nfrom __future__ import division, print_function, absolute_import\n\nimport collections\nimport os\nimport random\nimport urllib\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\nlearning_rate = 0.1\nbatch_size = 128\nnum_steps = 3000000\ndisplay_step = 10000\neval_step = 200000\n\neval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n\nembedding_size = 200 max_vocabulary_size = 50000 min_occurrence = 10 skip_window = 3 num_skips = 2 num_sampled = 64 \nurl = 'http://mattmahoney.net/dc/text8.zip'\ndata_path = 'text8.zip'\nif not os.path.exists(data_path):\n    print(\"Downloading the dataset... (It may take some time)\")\n    filename, _ = urllib.urlretrieve(url, data_path)\n    print(\"Done!\")\n================\n        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n\ntest_x, test_y = mnist.test.images, mnist.test.labels\nprint(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))\nfrom __future__ import division, print_function, absolute_import\n\nimport collections\nimport os\nimport random\nimport urllib\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\nlearning_rate = 0.1\nbatch_size = 128\nnum_steps = 3000000\ndisplay_step = 10000\neval_step = 200000\n\neval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n\nembedding_size = 200 max_vocabulary_size = 50000 min_occurrence = 10 skip_window = 3 num_skips = 2 num_sampled = 64 \nurl = 'http://mattmahoney.net/dc/text8.zip'\ndata_path = 'text8.zip'\nif not os.path.exists(data_path):\n    print(\"Downloading the dataset... (It may take some time)\")\n    filename, _ = urllib.urlretrieve(url, data_path)\n    print(\"Done!\")\nwith zipfile.ZipFile(data_path) as f:\n    text_words = f.read(f.namelist()[0]).lower().split()\ncount = [('UNK', -1)]\ncount.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\nfor i in range(len(count) - 1, -1, -1):\n    if count[i][1] < min_occurrence:\n        count.pop(i)\n    else:\n                break\nvocabulary_size = len(count)\n================\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\nlearning_rate = 0.1\nbatch_size = 128\nnum_steps = 3000000\ndisplay_step = 10000\neval_step = 200000\n\neval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n\nembedding_size = 200 max_vocabulary_size = 50000 min_occurrence = 10 skip_window = 3 num_skips = 2 num_sampled = 64 \nurl = 'http://mattmahoney.net/dc/text8.zip'\ndata_path = 'text8.zip'\nif not os.path.exists(data_path):\n    print(\"Downloading the dataset... (It may take some time)\")\n    filename, _ = urllib.urlretrieve(url, data_path)\n    print(\"Done!\")\nwith zipfile.ZipFile(data_path) as f:\n    text_words = f.read(f.namelist()[0]).lower().split()\ncount = [('UNK', -1)]\ncount.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\nfor i in range(len(count) - 1, -1, -1):\n    if count[i][1] < min_occurrence:\n        count.pop(i)\n    else:\n                break\nvocabulary_size = len(count)\nword2id = dict()\nfor i, (word, _)in enumerate(count):\n    word2id[word] = i\n\ndata = list()\nunk_count = 0\nfor word in text_words:\n        index = word2id.get(word, 0)\n    if index == 0:\n        unk_count += 1\n================\neval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n\nembedding_size = 200 max_vocabulary_size = 50000 min_occurrence = 10 skip_window = 3 num_skips = 2 num_sampled = 64 \nurl = 'http://mattmahoney.net/dc/text8.zip'\ndata_path = 'text8.zip'\nif not os.path.exists(data_path):\n    print(\"Downloading the dataset... (It may take some time)\")\n    filename, _ = urllib.urlretrieve(url, data_path)\n    print(\"Done!\")\nwith zipfile.ZipFile(data_path) as f:\n    text_words = f.read(f.namelist()[0]).lower().split()\ncount = [('UNK', -1)]\ncount.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\nfor i in range(len(count) - 1, -1, -1):\n    if count[i][1] < min_occurrence:\n        count.pop(i)\n    else:\n                break\nvocabulary_size = len(count)\nword2id = dict()\nfor i, (word, _)in enumerate(count):\n    word2id[word] = i\n\ndata = list()\nunk_count = 0\nfor word in text_words:\n        index = word2id.get(word, 0)\n    if index == 0:\n        unk_count += 1\n    data.append(index)\ncount[0] = ('UNK', unk_count)\nid2word = dict(zip(word2id.values(), word2id.keys()))\n\nprint(\"Words count:\", len(text_words))\nprint(\"Unique words:\", len(set(text_words)))\nprint(\"Vocabulary size:\", vocabulary_size)\nprint(\"Most common words:\", count[:10])\ndata_index = 0\ndef next_batch(batch_size, num_skips, skip_window):\n================\n    text_words = f.read(f.namelist()[0]).lower().split()\ncount = [('UNK', -1)]\ncount.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\nfor i in range(len(count) - 1, -1, -1):\n    if count[i][1] < min_occurrence:\n        count.pop(i)\n    else:\n                break\nvocabulary_size = len(count)\nword2id = dict()\nfor i, (word, _)in enumerate(count):\n    word2id[word] = i\n\ndata = list()\nunk_count = 0\nfor word in text_words:\n        index = word2id.get(word, 0)\n    if index == 0:\n        unk_count += 1\n    data.append(index)\ncount[0] = ('UNK', unk_count)\nid2word = dict(zip(word2id.values(), word2id.keys()))\n\nprint(\"Words count:\", len(text_words))\nprint(\"Unique words:\", len(set(text_words)))\nprint(\"Vocabulary size:\", vocabulary_size)\nprint(\"Most common words:\", count[:10])\ndata_index = 0\ndef next_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n        span = 2 * skip_window + 1\n    buffer = collections.deque(maxlen=span)\n    if data_index + span > len(data):\n        data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n================\nfor i, (word, _)in enumerate(count):\n    word2id[word] = i\n\ndata = list()\nunk_count = 0\nfor word in text_words:\n        index = word2id.get(word, 0)\n    if index == 0:\n        unk_count += 1\n    data.append(index)\ncount[0] = ('UNK', unk_count)\nid2word = dict(zip(word2id.values(), word2id.keys()))\n\nprint(\"Words count:\", len(text_words))\nprint(\"Unique words:\", len(set(text_words)))\nprint(\"Vocabulary size:\", vocabulary_size)\nprint(\"Most common words:\", count[:10])\ndata_index = 0\ndef next_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n        span = 2 * skip_window + 1\n    buffer = collections.deque(maxlen=span)\n    if data_index + span > len(data):\n        data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n    data_index += span\n    for i in range(batch_size // num_skips):\n        context_words = [w for w in range(span) if w != skip_window]\n        words_to_use = random.sample(context_words, num_skips)\n        for j, context_word in enumerate(words_to_use):\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[context_word]\n        if data_index == len(data):\n            buffer.extend(data[0:span])\n            data_index = span\n================\ncount[0] = ('UNK', unk_count)\nid2word = dict(zip(word2id.values(), word2id.keys()))\n\nprint(\"Words count:\", len(text_words))\nprint(\"Unique words:\", len(set(text_words)))\nprint(\"Vocabulary size:\", vocabulary_size)\nprint(\"Most common words:\", count[:10])\ndata_index = 0\ndef next_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n        span = 2 * skip_window + 1\n    buffer = collections.deque(maxlen=span)\n    if data_index + span > len(data):\n        data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n    data_index += span\n    for i in range(batch_size // num_skips):\n        context_words = [w for w in range(span) if w != skip_window]\n        words_to_use = random.sample(context_words, num_skips)\n        for j, context_word in enumerate(words_to_use):\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[context_word]\n        if data_index == len(data):\n            buffer.extend(data[0:span])\n            data_index = span\n        else:\n            buffer.append(data[data_index])\n            data_index += 1\n        data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels\nX = tf.placeholder(tf.int32, shape=[None])\nY = tf.placeholder(tf.int32, shape=[None, 1])\n\nwith tf.device('/cpu:0'):\n        embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n================\n"
    }
   ],
   "source": [
    "for i in range(0, 50):\n",
    "    print(github.loc[i, '0'])\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ims=True)\nW1 += -LEARNING_RATE * dW1\nb1 += -LEARNING_RATE * db1\nW2 += -LEARNING_RATE * dW2\nb2 += -LEARNING_RATE * db2\nX_train = X_train.numpy()\ny_train = y_train.numpy()\nX_val = X_val.numpy()\ny_val = y_val.numpy()\nX_test = X_test.numpy()\ny_test = y_test.numpy()\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\n-----------------------------\nLABEL:  0\n=============================\n=============================\nprint (f\"b2: {b2.shape}\")\nlogits = np.dot(a1, W2) + b2\nprint (f\"logits: {logits.shape}\")\nprint (f\"sample: {logits[0]}\")\nexp_logits = np.exp(logits)\ny_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\nprint (f\"y_hat: {y_hat.shape}\")\nprint (f\"sample: {y_hat[0]}\")\ncorrect_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\nloss = np.sum(correct_class_logprobs) / len(y_train)\ndscores = y_hat\ndscores[range(len(y_hat)), y_train] -= 1\ndscores /= len(y_train)\ndW2 = np.dot(a1.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)\ndhidden = np.dot(dscores, W2.T)\ndhidden[a1 <= 0] = 0 dW1 = np.dot(X_train.T, dhidden)\ndb1 = np.sum(dhidden, axis=0, keepdims=True)\nW1 += -LEARNING_RATE * dW1\nb1 += -LEARNING_RATE * db1\nW2 += -LEARNING_RATE * dW2\nb2 += -LEARNING_RATE * db2\nX_train = X_train.numpy()\ny_train = y_train.numpy()\nX_val = X_val.numpy()\ny_val = y_val.numpy()\nX_test = X_test.numpy()\ny_test = y_test.numpy()\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\nb1 = np.zeros((1, HIDDEN_DIM))\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\n\nfor epoch_num in range(1000):\n\n        z1 = np.dot(X_train, W1) + b1\n\n        a1 = np.maximum(0, z1) \n        logits = np.dot(a1, W2) + b2\n-----------------------------\nLABEL:  0\n=============================\n=============================\ndscores = y_hat\ndscores[range(len(y_hat)), y_train] -= 1\ndscores /= len(y_train)\ndW2 = np.dot(a1.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)\ndhidden = np.dot(dscores, W2.T)\ndhidden[a1 <= 0] = 0 dW1 = np.dot(X_train.T, dhidden)\ndb1 = np.sum(dhidden, axis=0, keepdims=True)\nW1 += -LEARNING_RATE * dW1\nb1 += -LEARNING_RATE * db1\nW2 += -LEARNING_RATE * dW2\nb2 += -LEARNING_RATE * db2\nX_train = X_train.numpy()\ny_train = y_train.numpy()\nX_val = X_val.numpy()\ny_val = y_val.numpy()\nX_test = X_test.numpy()\ny_test = y_test.numpy()\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\nb1 = np.zeros((1, HIDDEN_DIM))\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\n\nfor epoch_num in range(1000):\n\n        z1 = np.dot(X_train, W1) + b1\n\n        a1 = np.maximum(0, z1) \n        logits = np.dot(a1, W2) + b2\n    \n        exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n        correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n        if epoch_num%100 == 0:\n                y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n-----------------------------\nLABEL:  0\n=============================\n=============================\nW2 += -LEARNING_RATE * dW2\nb2 += -LEARNING_RATE * db2\nX_train = X_train.numpy()\ny_train = y_train.numpy()\nX_val = X_val.numpy()\ny_val = y_val.numpy()\nX_test = X_test.numpy()\ny_test = y_test.numpy()\nW1 = 0.01 * np.random.randn(INPUT_DIM, HIDDEN_DIM)\nb1 = np.zeros((1, HIDDEN_DIM))\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\n\nfor epoch_num in range(1000):\n\n        z1 = np.dot(X_train, W1) + b1\n\n        a1 = np.maximum(0, z1) \n        logits = np.dot(a1, W2) + b2\n    \n        exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n        correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n        if epoch_num%100 == 0:\n                y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n\n        dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW2 = np.dot(a1.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n\n        dhidden = np.dot(dscores, W2.T)\n    dhidden[a1 <= 0] = 0     dW1 = np.dot(X_train.T, dhidden)\n-----------------------------\nLABEL:  0\n=============================\n=============================\nW2 = 0.01 * np.random.randn(HIDDEN_DIM, NUM_CLASSES)\nb2 = np.zeros((1, NUM_CLASSES))\n\nfor epoch_num in range(1000):\n\n        z1 = np.dot(X_train, W1) + b1\n\n        a1 = np.maximum(0, z1) \n        logits = np.dot(a1, W2) + b2\n    \n        exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n        correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n        if epoch_num%100 == 0:\n                y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n\n        dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW2 = np.dot(a1.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n\n        dhidden = np.dot(dscores, W2.T)\n    dhidden[a1 <= 0] = 0     dW1 = np.dot(X_train.T, dhidden)\n    db1 = np.sum(dhidden, axis=0, keepdims=True)\n\n        W1 += -1e0 * dW1\n    b1 += -1e0 * db1\n    W2 += -1e0 * dW2\n    b2 += -1e0 * db2\nmodel = MLPFromScratch()\nlogits_train = model.predict(X_train)\npred_train = np.argmax(logits_train, axis=1)\nlogits_test = model.predict(X_test)\n-----------------------------\nLABEL:  0\n=============================\n=============================\n        exp_logits = np.exp(logits)\n    y_hat = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n        correct_class_logprobs = -np.log(y_hat[range(len(y_hat)), y_train])\n    loss = np.sum(correct_class_logprobs) / len(y_train)\n\n        if epoch_num%100 == 0:\n                y_pred = np.argmax(logits, axis=1)\n        accuracy =  np.mean(np.equal(y_train, y_pred))\n        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n\n        dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW2 = np.dot(a1.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n\n        dhidden = np.dot(dscores, W2.T)\n    dhidden[a1 <= 0] = 0     dW1 = np.dot(X_train.T, dhidden)\n    db1 = np.sum(dhidden, axis=0, keepdims=True)\n\n        W1 += -1e0 * dW1\n    b1 += -1e0 * db1\n    W2 += -1e0 * dW2\n    b2 += -1e0 * db2\nmodel = MLPFromScratch()\nlogits_train = model.predict(X_train)\npred_train = np.argmax(logits_train, axis=1)\nlogits_test = model.predict(X_test)\npred_test = np.argmax(logits_test, axis=1)\ntrain_acc =  np.mean(np.equal(y_train, pred_train))\ntest_acc = np.mean(np.equal(y_test, pred_test))\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\ndef plot_multiclass_decision_boundary_numpy(model, X, y, savefig_fp=None):\n    \n        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n        dscores = y_hat\n    dscores[range(len(y_hat)), y_train] -= 1\n    dscores /= len(y_train)\n    dW2 = np.dot(a1.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n\n        dhidden = np.dot(dscores, W2.T)\n    dhidden[a1 <= 0] = 0     dW1 = np.dot(X_train.T, dhidden)\n    db1 = np.sum(dhidden, axis=0, keepdims=True)\n\n        W1 += -1e0 * dW1\n    b1 += -1e0 * db1\n    W2 += -1e0 * dW2\n    b2 += -1e0 * db2\nmodel = MLPFromScratch()\nlogits_train = model.predict(X_train)\npred_train = np.argmax(logits_train, axis=1)\nlogits_test = model.predict(X_test)\npred_test = np.argmax(logits_test, axis=1)\ntrain_acc =  np.mean(np.equal(y_train, pred_train))\ntest_acc = np.mean(np.equal(y_test, pred_test))\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\ndef plot_multiclass_decision_boundary_numpy(model, X, y, savefig_fp=None):\n    \n        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n\n        x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n        plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n        W1 += -1e0 * dW1\n    b1 += -1e0 * db1\n    W2 += -1e0 * dW2\n    b2 += -1e0 * db2\nmodel = MLPFromScratch()\nlogits_train = model.predict(X_train)\npred_train = np.argmax(logits_train, axis=1)\nlogits_test = model.predict(X_test)\npred_test = np.argmax(logits_test, axis=1)\ntrain_acc =  np.mean(np.equal(y_train, pred_train))\ntest_acc = np.mean(np.equal(y_test, pred_test))\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\ndef plot_multiclass_decision_boundary_numpy(model, X, y, savefig_fp=None):\n    \n        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n\n        x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n        plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n        if savefig_fp:\n        plt.savefig(savefig_fp, format='png')\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)\nplt.show()\n-----------------------------\nLABEL:  1\n=============================\n=============================\ntrain_acc =  np.mean(np.equal(y_train, pred_train))\ntest_acc = np.mean(np.equal(y_test, pred_test))\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\ndef plot_multiclass_decision_boundary_numpy(model, X, y, savefig_fp=None):\n    \n        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101),\n                         np.linspace(y_min, y_max, 101))\n\n        x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n        plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n        if savefig_fp:\n        plt.savefig(savefig_fp, format='png')\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)\nplt.show()\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, x_in, apply_softmax=False):\n        z = F.relu(self.fc1(x_in))         y_pred = self.fc2(z)\n        if apply_softmax:\n            y_pred = F.softmax(y_pred, dim=1) \n-----------------------------\nLABEL:  1\n=============================\n=============================\n        x_in = np.c_[xx.ravel(), yy.ravel()]\n    y_pred = model.predict(x_in)\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n\n        plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n        if savefig_fp:\n        plt.savefig(savefig_fp, format='png')\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)\nplt.show()\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, x_in, apply_softmax=False):\n        z = F.relu(self.fc1(x_in))         y_pred = self.fc2(z)\n        if apply_softmax:\n            y_pred = F.softmax(y_pred, dim=1) \n        return y_pred\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\nsummary(model, input_size=(INPUT_DIM,))\nweights = torch.Tensor([class_weights[key] for key in sorted(class_weights.keys())])\nloss_fn = nn.CrossEntropyLoss(weight=weights)\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        plt.savefig(savefig_fp, format='png')\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary_numpy(model=model, X=X_test, y=y_test)\nplt.show()\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, x_in, apply_softmax=False):\n        z = F.relu(self.fc1(x_in))         y_pred = self.fc2(z)\n        if apply_softmax:\n            y_pred = F.softmax(y_pred, dim=1) \n        return y_pred\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\nsummary(model, input_size=(INPUT_DIM,))\nweights = torch.Tensor([class_weights[key] for key in sorted(class_weights.keys())])\nloss_fn = nn.CrossEntropyLoss(weight=weights)\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE) \nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\nfor epoch in range(NUM_EPOCHS*10):\n        y_pred = model(X_train)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n    def forward(self, x_in, apply_softmax=False):\n        z = F.relu(self.fc1(x_in))         y_pred = self.fc2(z)\n        if apply_softmax:\n            y_pred = F.softmax(y_pred, dim=1) \n        return y_pred\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\nsummary(model, input_size=(INPUT_DIM,))\nweights = torch.Tensor([class_weights[key] for key in sorted(class_weights.keys())])\nloss_fn = nn.CrossEntropyLoss(weight=weights)\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE) \nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\nfor epoch in range(NUM_EPOCHS*10):\n        y_pred = model(X_train)\n\n        loss = loss_fn(y_pred, y_train)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n    if epoch%10==0: \n        predictions = y_pred.max(dim=1)[1]         accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n-----------------------------\nLABEL:  0\n=============================\n=============================\nmodel = MLP(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\nprint (model.named_parameters)\nsummary(model, input_size=(INPUT_DIM,))\nweights = torch.Tensor([class_weights[key] for key in sorted(class_weights.keys())])\nloss_fn = nn.CrossEntropyLoss(weight=weights)\ndef accuracy_fn(y_pred, y_true):\n    n_correct = torch.eq(y_pred, y_true).sum().item()\n    accuracy = (n_correct / len(y_pred)) * 100\n    return accuracy\noptimizer = Adam(model.parameters(), lr=LEARNING_RATE) \nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\nfor epoch in range(NUM_EPOCHS*10):\n        y_pred = model(X_train)\n\n        loss = loss_fn(y_pred, y_train)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n    if epoch%10==0: \n        predictions = y_pred.max(dim=1)[1]         accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\npred_train = model(X_train, apply_softmax=True)\npred_test = model(X_test, apply_softmax=True)\nprint (f\"sample probability: {pred_test[0]}\")\npred_train = pred_train.max(dim=1)[1]\npred_test = pred_test.max(dim=1)[1]\nprint (f\"sample class: {pred_test[0]}\")\ntrain_acc = accuracy_score(y_train, pred_train)\ntest_acc = accuracy_score(y_test, pred_test)\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\n-----------------------------\nLABEL:  1\n=============================\n=============================\nX_train = torch.Tensor(X_train)\ny_train = torch.LongTensor(y_train)\nX_val = torch.Tensor(X_val)\ny_val = torch.LongTensor(y_val)\nX_test = torch.Tensor(X_test)\ny_test = torch.LongTensor(y_test)\nfor epoch in range(NUM_EPOCHS*10):\n        y_pred = model(X_train)\n\n        loss = loss_fn(y_pred, y_train)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n    if epoch%10==0: \n        predictions = y_pred.max(dim=1)[1]         accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)\n        print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\npred_train = model(X_train, apply_softmax=True)\npred_test = model(X_test, apply_softmax=True)\nprint (f\"sample probability: {pred_test[0]}\")\npred_train = pred_train.max(dim=1)[1]\npred_test = pred_test.max(dim=1)[1]\nprint (f\"sample class: {pred_test[0]}\")\ntrain_acc = accuracy_score(y_train, pred_train)\ntest_acc = accuracy_score(y_test, pred_test)\nprint (f\"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}\")\nplot_confusion_matrix(y_test, pred_test, classes=classes)\nprint (classification_report(y_test, pred_test))\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\nplt.show()\n-----------------------------\nLABEL:  0\n=============================\n=============================\n"
    }
   ],
   "source": [
    "for i in range(1200, 1300):\n",
    "    print(results.loc[i, '0'])\n",
    "    print('-----------------------------')\n",
    "    print('LABEL: ', results.loc[i, 'preprocessing'])\n",
    "    print('=============================')\n",
    "    print('=============================')"
   ]
  }
 ]
}