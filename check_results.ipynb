{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/github_chunks_10_preprocessing_logreg_v3.1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=========================\n=============================\nweights = {\n    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n}\nbiases = {\n    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n}\ndef encoder(x):\n        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n                                   biases['encoder_b1']))\n        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n                                   biases['encoder_b2']))\n    return layer_2\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\ndef decoder(x):\n        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n                                   biases['decoder_b1']))\n        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n                                   biases['decoder_b2']))\n    return layer_2\n\nencoder_op = encoder(X)\ndecoder_op = decoder(encoder_op)\n-----------------------------\nLABEL:  1\n=============================\n=============================\ny_pred = decoder_op\ny_true = X\n\nloss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\noptimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\nfor i in range(1, num_steps+1):\n            batch_x, _ = mnist.train.next_batch(batch_size)\n\n        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n        if i % display_step == 0 or i == 1:\n        print('Step %i: Minibatch Loss: %f' % (i, l))\nn = 4\ncanvas_orig = np.empty((28 * n, 28 * n))\n-----------------------------\nLABEL:  1\n=============================\n=============================\nfor i in range(n):\n        batch_x, _ = mnist.test.next_batch(n)\n        g = sess.run(decoder_op, feed_dict={X: batch_x})\n    \n        for j in range(n):\n                canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])\n        for j in range(n):\n                canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\nplt.figure(figsize=(n, n))\nplt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\nplt.show()\n\nprint(\"Reconstructed Images\")\nplt.figure(figsize=(n, n))\nplt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\nplt.show()\nfrom __future__ import print_function\n-----------------------------\nLABEL:  1\n=============================\n=============================\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\nlearning_rate = 0.001\ntraining_steps = 10000\nbatch_size = 128\n-----------------------------\nLABEL:  0\n=============================\n=============================\n\nnum_input = 28 timesteps = 28 num_hidden = 128 num_classes = 10 \nX = tf.placeholder(\"float\", [None, timesteps, num_input])\nY = tf.placeholder(\"float\", [None, num_classes])\nweights = {\n        'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([num_classes]))\n-----------------------------\nLABEL:  1\n=============================\n=============================\ndef BiRNN(x, weights, biases):\n\n            \n        x = tf.unstack(x, timesteps, 1)\n\n            lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n        lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\n        try:\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                                              dtype=tf.float32)\n    except Exception:         outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                        dtype=tf.float32)\n\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\nlogits = BiRNN(X, weights, biases)\nprediction = tf.nn.softmax(logits)\n\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n-----------------------------\nLABEL:  1\n=============================\n=============================\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n    for step in range(1, training_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n                batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n                        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" +                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" +                  \"{:.3f}\".format(acc))\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    print(\"Optimization Finished!\")\n\n        test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n    test_label = mnist.test.labels[:test_len]\n    print(\"Testing Accuracy:\",        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n\nfrom __future__ import division, print_function, absolute_import\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nlearning_rate = 0.001\nnum_steps = 2000\nbatch_size = 128\n\n-----------------------------\nLABEL:  0\n=============================\n=============================\ndef conv_net(x_dict, n_classes, dropout, reuse, is_training):\n    \n        with tf.variable_scope('ConvNet', reuse=reuse):\n                x = x_dict['images']\n\n                                x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n                conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n                conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n                conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n\n                fc1 = tf.contrib.layers.flatten(conv2)\n\n                fc1 = tf.layers.dense(fc1, 1024)\n                fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n\n                out = tf.layers.dense(fc1, n_classes)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    return out\ndef model_fn(features, labels, mode):\n    \n                logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n    \n        pred_classes = tf.argmax(logits_test, axis=1)\n    pred_probas = tf.nn.softmax(logits_test)\n    \n-----------------------------\nLABEL:  1\n=============================\n=============================\n        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n        \n        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n    \n        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n    \n-----------------------------\nLABEL:  1\n=============================\n=============================\n      mode=mode,\n      predictions=pred_classes,\n      loss=loss_op,\n      train_op=train_op,\n      eval_metric_ops={'accuracy': acc_op})\n\n    return estim_specs\nmodel = tf.estimator.Estimator(model_fn)\ninput_fn = tf.estimator.inputs.numpy_input_fn(\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    batch_size=batch_size, num_epochs=None, shuffle=True)\nmodel.train(input_fn, steps=num_steps)\ninput_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\nn_images = 4\ntest_images = mnist.test.images[:n_images]\ninput_fn = tf.estimator.inputs.numpy_input_fn(\n-----------------------------\nLABEL:  0\n=============================\n=============================\npreds = list(model.predict(input_fn))\n\nfor i in range(n_images):\n    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n    plt.show()\n    print(\"Model prediction:\", preds[i])\nfrom __future__ import division, print_function, absolute_import\n\nimport tensorflow as tf\n-----------------------------\nLABEL:  1\n=============================\n=============================\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\nlearning_rate = 0.001\nnum_steps = 500\nbatch_size = 128\ndisplay_step = 10\n\nnum_input = 784 num_classes = 10 dropout = 0.75 \nX = tf.placeholder(tf.float32, [None, num_input])\n-----------------------------\nLABEL:  1\n=============================\n=============================\nkeep_prob = tf.placeholder(tf.float32) \ndef conv2d(x, W, b, strides=1):\n        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool2d(x, k=2):\n        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n\ndef conv_net(x, weights, biases, dropout):\n                x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n        conv1 = maxpool2d(conv1, k=2)\n\n        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\n            fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n        fc1 = tf.nn.dropout(fc1, dropout)\n\n        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    return out\nweights = {\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n        'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n        'out': tf.Variable(tf.random_normal([1024, num_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n-----------------------------\nLABEL:  1\n=============================\n=============================\n}\n\nlogits = conv_net(X, weights, biases, keep_prob)\nprediction = tf.nn.softmax(logits)\n\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n\n        sess.run(init)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n        if step % display_step == 0 or step == 1:\n                        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y,\n                                                                 keep_prob: 1.0})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" +                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" +                  \"{:.3f}\".format(acc))\n\n    print(\"Optimization Finished!\")\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        print(\"Testing Accuracy:\",        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n                                      Y: mnist.test.labels[:256],\n                                      keep_prob: 1.0}))\n\nfrom __future__ import division, print_function, absolute_import\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n-----------------------------\nLABEL:  0\n=============================\n=============================\nbatch_size = 128\nlr_generator = 0.002\nlr_discriminator = 0.002\n\nimage_dim = 784 noise_dim = 100 \nnoise_input = tf.placeholder(tf.float32, shape=[None, noise_dim])\nreal_image_input = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\nis_training = tf.placeholder(tf.bool)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\n    return 0.5 * (1 + alpha) * x + 0.5 * (1 - alpha) * abs(x)\n\ndef generator(x, reuse=False):\n    with tf.variable_scope('Generator', reuse=reuse):\n                        x = tf.layers.dense(x, units=7 * 7 * 128)\n        x = tf.layers.batch_normalization(x, training=is_training)\n        x = tf.nn.relu(x)\n                        x = tf.reshape(x, shape=[-1, 7, 7, 128])\n                x = tf.layers.conv2d_transpose(x, 64, 5, strides=2, padding='same')\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        x = tf.nn.relu(x)\n                x = tf.layers.conv2d_transpose(x, 1, 5, strides=2, padding='same')\n                x = tf.nn.tanh(x)\n        return x\n\n\ndef discriminator(x, reuse=False):\n    with tf.variable_scope('Discriminator', reuse=reuse):\n                x = tf.layers.conv2d(x, 64, 5, strides=2, padding='same')\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        x = leakyrelu(x)\n        x = tf.layers.conv2d(x, 128, 5, strides=2, padding='same')\n        x = tf.layers.batch_normalization(x, training=is_training)\n        x = leakyrelu(x)\n                x = tf.reshape(x, shape=[-1, 7*7*128])\n        x = tf.layers.dense(x, 1024)\n        x = tf.layers.batch_normalization(x, training=is_training)\n        x = leakyrelu(x)\n                x = tf.layers.dense(x, 2)\n-----------------------------\nLABEL:  1\n=============================\n=============================\n\ngen_sample = generator(noise_input)\n\ndisc_real = discriminator(real_image_input)\ndisc_fake = discriminator(gen_sample, reuse=True)\n\nstacked_gan = discriminator(gen_sample, reuse=True)\n\ndisc_loss_real = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n-----------------------------\nLABEL:  1\n=============================\n=============================\ndisc_loss_fake = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=disc_fake, labels=tf.zeros([batch_size], dtype=tf.int32)))\ndisc_loss = disc_loss_real + disc_loss_fake\ngen_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=stacked_gan, labels=tf.ones([batch_size], dtype=tf.int32)))\n\noptimizer_gen = tf.train.AdamOptimizer(learning_rate=lr_generator, beta1=0.5, beta2=0.999)\noptimizer_disc = tf.train.AdamOptimizer(learning_rate=lr_discriminator, beta1=0.5, beta2=0.999)\n\n-----------------------------\nLABEL:  1\n=============================\n=============================\ndisc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n\ngen_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')\nwith tf.control_dependencies(gen_update_ops):\n    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\ndisc_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Discriminator')\nwith tf.control_dependencies(disc_update_ops):\n    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n    \n-----------------------------\nLABEL:  1\n=============================\n=============================\nsess = tf.Session()\n\nsess.run(init)\n    \nfor i in range(1, num_steps+1):\n\n            batch_x, _ = mnist.train.next_batch(batch_size)\n    batch_x = np.reshape(batch_x, newshape=[-1, 28, 28, 1])\n        batch_x = batch_x * 2. - 1.\n-----------------------------\nLABEL:  1\n=============================\n=============================\n            z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n    _, dl = sess.run([train_disc, disc_loss], feed_dict={real_image_input: batch_x, noise_input: z, is_training:True})\n    \n            z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n    _, gl = sess.run([train_gen, gen_loss], feed_dict={noise_input: z, is_training:True})\n    \n    if i % 500 == 0 or i == 1:\n        print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\nn = 6\n-----------------------------\nLABEL:  0\n=============================\n=============================\nfor i in range(n):\n        z = np.random.uniform(-1., 1., size=[n, noise_dim])\n        g = sess.run(gen_sample, feed_dict={noise_input: z, is_training:False})\n        g = (g + 1.) / 2.\n        g = -1 * (g - 1)\n    for j in range(n):\n                canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n\nplt.figure(figsize=(n, n))\n-----------------------------\nLABEL:  1\n=============================\n=============================\nplt.show()\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport random\n\nclass ToySequenceData(object):\n    \n    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n-----------------------------\nLABEL:  1\n=============================\n=============================\n        self.data = []\n        self.labels = []\n        self.seqlen = []\n        for i in range(n_samples):\n                        len = random.randint(min_seq_len, max_seq_len)\n                        self.seqlen.append(len)\n                        if random.random() < .5:\n                                rand_start = random.randint(0, max_value - len)\n                s = [[float(i)/max_value] for i in\n-----------------------------\nLABEL:  1\n=============================\n=============================\n                                s += [[0.] for i in range(max_seq_len - len)]\n                self.data.append(s)\n                self.labels.append([1., 0.])\n            else:\n                                s = [[float(random.randint(0, max_value))/max_value]\n                     for i in range(len)]\n                                s += [[0.] for i in range(max_seq_len - len)]\n                self.data.append(s)\n                self.labels.append([0., 1.])\n-----------------------------\nLABEL:  1\n=============================\n=============================\n"
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(results.loc[i, '0'])\n",
    "    print('-----------------------------')\n",
    "    print('LABEL: ', results.loc[i, 'preprocessing'])\n",
    "    print('=============================')\n",
    "    print('=============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}